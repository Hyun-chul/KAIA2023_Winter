{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Many2Many_Char_RNN.ipynb","provenance":[{"file_id":"1VqtUYVX8YSmfACYmyUnv_eLIl1eNTY00","timestamp":1652519198632}],"collapsed_sections":[],"authorship_tag":"ABX9TyNuu4dx3Kat2kUQDGRUO5z/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZhoMnn88Hp6R"},"source":["참고문헌: PyTorch로 시작하는 딥 러닝 입문, 파이썬 딥러닝 파이토치 (이경택, 방성수, 안상준 지음), 정보문화사"]},{"cell_type":"markdown","metadata":{"id":"8EK4VR7DHbR5"},"source":["# 11. 다대다 RNN을 이용한 텍스트 생성\n"]},{"cell_type":"markdown","metadata":{"id":"20s4gsQjHdyv"},"source":["## 01. 문자 단위 RNN(Char RNN)"]},{"cell_type":"markdown","metadata":{"id":"cMgXNGfeHhK9"},"source":["이번 챕터에서는 모든 시점의 입력에 대해서 모든 시점에 대해서 출력을 하는 다대다 RNN을 구현해봅시다. 다대다 RNN은 대표적으로 품사 태깅, 개체명 인식 등에서 사용됩니다."]},{"cell_type":"markdown","metadata":{"id":"v0AndLccHiaO"},"source":["## 1. 문자 단위 RNN(Char RNN)\n"]},{"cell_type":"markdown","metadata":{"id":"KTFCs8K3Hjna"},"source":["RNN의 입출력의 단위가 단어 레벨(word-level)이 아니라 문자 레벨(character-level)로 하여 RNN을 구현한다면, 이를 문자 단위 RNN이라고 합니다. RNN 구조 자체가 달라진 것은 아니고, 입, 출력의 단위가 문자로 바뀌었을 뿐입니다. 문자 단위 RNN을 다대다 구조로 구현해봅시다."]},{"cell_type":"markdown","metadata":{"id":"kfXLrmQ5Hk1Y"},"source":["우선 필요한 도구들을 임포트합니다."]},{"cell_type":"code","metadata":{"id":"8bm0auFdHmFM"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QJNf_M11Hr99"},"source":["## 1. 훈련 데이터 전처리하기"]},{"cell_type":"markdown","metadata":{"id":"pUhg6zkJHtlc"},"source":["여기서는 문자 시퀀스 apple을 입력받으면 pple!를 출력하는 RNN을 구현해볼 겁니다. 이렇게 구현하는 어떤 의미가 있지는 않습니다. 그저 RNN의 동작을 이해하기 위한 목적입니다.\n","\n","입력 데이터와 레이블 데이터에 대해서 문자 집합(voabulary)을 만듭니다. 여기서 문자 집합은 중복을 제거한 문자들의 집합입니다."]},{"cell_type":"code","metadata":{"id":"qRVP-CLEHvXw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794636937,"user_tz":-540,"elapsed":24,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"ce5d21bf-5f86-4d95-baf3-356efde66c5b"},"source":["input_str = 'apple'\n","label_str = 'pple!'\n","char_vocab = sorted(list(set(input_str+label_str)))\n","vocab_size = len(char_vocab)\n","print ('문자 집합의 크기 : {}'.format(vocab_size))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["문자 집합의 크기 : 5\n"]}]},{"cell_type":"markdown","metadata":{"id":"cmZjVs7AHwfd"},"source":["현재 문자 집합에는 총 5개의 문자가 있습니다. !, a, e, l, p입니다. 이제 하이퍼파라미터를 정의해줍니다. 이때 입력은 원-핫 벡터를 사용할 것이므로 입력의 크기는 문자 집합의 크기여야만 합니다."]},{"cell_type":"code","metadata":{"id":"V_U4VTtGHxts"},"source":["input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n","hidden_size = 5\n","output_size = 5\n","learning_rate = 0.1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NZYIwaNaHzP7"},"source":["이제 문자 집합에 고유한 정수를 부여합니다."]},{"cell_type":"code","metadata":{"id":"5Nk19M77H1rL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794636938,"user_tz":-540,"elapsed":20,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"9a2b3b9f-fa08-44f4-836b-a1a38196c971"},"source":["char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n","print(char_to_index)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"]}]},{"cell_type":"markdown","metadata":{"id":"gmIzaPBJH23J"},"source":["!은 0, a는 1, e는 2, l은 3, p는 4가 부여되었습니다. 나중에 예측 결과를 다시 문자 시퀀스로 보기위해서 반대로 정수로부터 문자를 얻을 수 있는 index_to_char을 만듭니다."]},{"cell_type":"code","metadata":{"id":"h6MmdMVwH4CW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794636938,"user_tz":-540,"elapsed":18,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"75ec3246-970c-47dd-a15b-53ef297a16cf"},"source":["index_to_char={}\n","for key, value in char_to_index.items():\n","    index_to_char[value] = key\n","print(index_to_char)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"]}]},{"cell_type":"markdown","metadata":{"id":"oh85JPUqH5Qi"},"source":["이제 입력 데이터와 레이블 데이터의 각 문자들을 정수로 맵핑합니다."]},{"cell_type":"code","metadata":{"id":"JPNKFnIfH62A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794636939,"user_tz":-540,"elapsed":17,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"f384b87f-31c7-4205-8430-1dca4fbd56c2"},"source":["x_data = [char_to_index[c] for c in input_str]\n","y_data = [char_to_index[c] for c in label_str]\n","print(x_data)\n","print(y_data)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 4, 4, 3, 2]\n","[4, 4, 3, 2, 0]\n"]}]},{"cell_type":"markdown","metadata":{"id":"oLqTb7dwH8Gq"},"source":["파이토치의 nn.RNN()은 기본적으로 3차원 텐서를 입력받습니다. 그렇기 때문에 배치 차원을 추가해줍니다."]},{"cell_type":"code","metadata":{"id":"JjrzV4IfH9sG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794636939,"user_tz":-540,"elapsed":16,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"fbbc46f6-06bd-48e2-8dd8-0e5782dc7db0"},"source":["# 배치 차원 추가\n","# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n","x_data = [x_data]\n","y_data = [y_data]\n","print(x_data)\n","print(y_data)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 4, 4, 3, 2]]\n","[[4, 4, 3, 2, 0]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"Fwdjxc2fH-2f"},"source":["입력 시퀀스의 각 문자들을 원-핫 벡터로 바꿔줍니다."]},{"cell_type":"code","metadata":{"id":"46PuPmqlIABP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794636940,"user_tz":-540,"elapsed":15,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"3ae0b7e8-c7fc-4777-e607-01ef36d018fa"},"source":["x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n","print(x_one_hot)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[array([[0., 1., 0., 0., 0.],\n","       [0., 0., 0., 0., 1.],\n","       [0., 0., 0., 0., 1.],\n","       [0., 0., 0., 1., 0.],\n","       [0., 0., 1., 0., 0.]])]\n"]}]},{"cell_type":"markdown","metadata":{"id":"6DCPxIkZIBLd"},"source":["입력 데이터와 레이블 데이터를 텐서로 바꿔줍니다."]},{"cell_type":"code","metadata":{"id":"BRVV3QuDICx-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794636941,"user_tz":-540,"elapsed":14,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"c56f66a2-8f33-4837-8cbf-8c67c9b2b4d9"},"source":["X = torch.FloatTensor(x_one_hot)\n","Y = torch.LongTensor(y_data)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  \"\"\"Entry point for launching an IPython kernel.\n"]}]},{"cell_type":"markdown","metadata":{"id":"BvEnRtj_ID42"},"source":["이제 각 텐서의 크기를 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"RHRgFWNjIFel","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794636941,"user_tz":-540,"elapsed":12,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"a0970bb2-3d6d-43e6-c9d0-6a5f7ba12a5e"},"source":["print('훈련 데이터의 크기 : {}'.format(X.shape))\n","print('레이블의 크기 : {}'.format(Y.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 데이터의 크기 : torch.Size([1, 5, 5])\n","레이블의 크기 : torch.Size([1, 5])\n"]}]},{"cell_type":"markdown","metadata":{"id":"b70wYTBlIGuy"},"source":["## 2. 모델 구현하기"]},{"cell_type":"markdown","metadata":{"id":"E6xePH3rIIRR"},"source":["이제 RNN 모델을 구현해봅시다. 아래에서 fc는 완전 연결층(fully-connected layer)을 의미하며 출력층으로 사용됩니다."]},{"cell_type":"code","metadata":{"id":"jOFIXW0UIJ6Z"},"source":["class Net(torch.nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(Net, self).__init__()\n","        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n","        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n","\n","    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n","        x, _status = self.rnn(x)\n","        x = self.fc(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qLo-TzihILVV"},"source":["클래스로 정의한 모델을 net에 저장합니다."]},{"cell_type":"code","metadata":{"id":"sOT56KeMINBN"},"source":["net = Net(input_size, hidden_size, output_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jJDtEWT0IOIL"},"source":["이제 입력된 모델에 입력을 넣어서 출력의 크기를 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"vhMKlsMSIP6Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794637303,"user_tz":-540,"elapsed":372,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"9ce706c5-ebed-48a1-a5c3-af93211e17fa"},"source":["outputs = net(X)\n","print(outputs.shape) # 3차원 텐서"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 5, 5])\n"]}]},{"cell_type":"markdown","metadata":{"id":"-sX4Cjl3IRFr"},"source":["(1, 5, 5)의 크기를 가지는데 각각 배치 차원, 시점(timesteps), 출력의 크기입니다. 나중에 정확도를 측정할 때는 이를 모두 펼쳐서 계산하게 되는데, 이때는 view를 사용하여 배치 차원과 시점 차원을 하나로 만듭니다."]},{"cell_type":"code","metadata":{"id":"4r8EiecfISsU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794637304,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"834a3656-e6b2-4f45-b13e-14cc297373ea"},"source":["print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 5])\n"]}]},{"cell_type":"markdown","metadata":{"id":"W6EKqtWGIUC8"},"source":["차원이 (5, 5)가 된 것을 볼 수 있습니다. 이제 레이블 데이터의 크기를 다시 복습봅시다."]},{"cell_type":"code","metadata":{"id":"phT2KQLIIVRD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794637304,"user_tz":-540,"elapsed":8,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"3f68c71c-a817-4ce6-97df-4879e2bbf3da"},"source":["print(Y.shape)\n","print(Y.view(-1).shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 5])\n","torch.Size([5])\n"]}]},{"cell_type":"markdown","metadata":{"id":"MCMNGs1SIWWE"},"source":["레이블 데이터는 (1, 5)의 크기를 가지는데, 마찬가지로 나중에 정확도를 측정할 때는 이걸 펼쳐서 계산할 예정입니다. 이 경우 (5)의 크기를 가지게 됩니다. 이제 옵티마이저와 손실 함수를 정의합니다."]},{"cell_type":"code","metadata":{"id":"8W7Vat63IXrO"},"source":["criterion = torch.nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hTFzsul6IZbz"},"source":["총 100번의 에포크를 학습합니다."]},{"cell_type":"code","metadata":{"id":"LVn20KIpIb7X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794638240,"user_tz":-540,"elapsed":941,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"3b73b8b9-41cd-4ded-da3d-b074b2f701da"},"source":["for i in range(100):\n","    optimizer.zero_grad()\n","    outputs = net(X)\n","    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n","    loss.backward() # 기울기 계산\n","    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n","\n","    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n","    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n","    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n","    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 loss:  1.7093403339385986 prediction:  [[2 3 3 2 3]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ellel\n","1 loss:  1.4315253496170044 prediction:  [[3 3 3 3 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  llll!\n","2 loss:  1.2190470695495605 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","3 loss:  0.9982198476791382 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","4 loss:  0.8037036061286926 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n","5 loss:  0.6518546342849731 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n","6 loss:  0.5290197134017944 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n","7 loss:  0.4172944128513336 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","8 loss:  0.32096797227859497 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","9 loss:  0.24315819144248962 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","10 loss:  0.17797502875328064 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","11 loss:  0.13196943700313568 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","12 loss:  0.0958857461810112 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","13 loss:  0.06875044107437134 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","14 loss:  0.05129484087228775 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","15 loss:  0.03946862369775772 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","16 loss:  0.030704837292432785 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","17 loss:  0.024145664647221565 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","18 loss:  0.019359100610017776 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","19 loss:  0.015952277928590775 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","20 loss:  0.013504509814083576 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","21 loss:  0.011662915349006653 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","22 loss:  0.010202610865235329 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","23 loss:  0.009005257859826088 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","24 loss:  0.008012073114514351 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","25 loss:  0.007187576033174992 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","26 loss:  0.00650213286280632 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","27 loss:  0.005926585290580988 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","28 loss:  0.005433978047221899 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","29 loss:  0.005002918187528849 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","30 loss:  0.004619329236447811 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","31 loss:  0.004275861196219921 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","32 loss:  0.003968749660998583 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","33 loss:  0.0036951948422938585 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","34 loss:  0.0034519582986831665 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","35 loss:  0.003235508454963565 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","36 loss:  0.003042307449504733 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","37 loss:  0.0028688814491033554 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","38 loss:  0.002712393179535866 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","39 loss:  0.002570501761510968 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","40 loss:  0.002441362477838993 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","41 loss:  0.002323485678061843 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","42 loss:  0.002215759828686714 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","43 loss:  0.0021172629203647375 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","44 loss:  0.002027143258601427 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","45 loss:  0.001944572664797306 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","46 loss:  0.0018690777942538261 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","47 loss:  0.0017998528201133013 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","48 loss:  0.0017363762017339468 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","49 loss:  0.001678031519986689 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","50 loss:  0.0016242486890405416 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","51 loss:  0.0015744585543870926 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","52 loss:  0.001528328051790595 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","53 loss:  0.0014853350585326552 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","54 loss:  0.0014452898176386952 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","55 loss:  0.0014079309767112136 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","56 loss:  0.0013729023048654199 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","57 loss:  0.0013400373281911016 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","58 loss:  0.0013092175358906388 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","59 loss:  0.0012803002027794719 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","60 loss:  0.001253095455467701 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","61 loss:  0.00122746080160141 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","62 loss:  0.0012032769154757261 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","63 loss:  0.0011804492678493261 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","64 loss:  0.0011587871704250574 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","65 loss:  0.0011382907396182418 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","66 loss:  0.0011187937343493104 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","67 loss:  0.0011001768289133906 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","68 loss:  0.0010824404889717698 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","69 loss:  0.001065489137545228 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","70 loss:  0.0010492755100131035 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","71 loss:  0.0010337044950574636 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","72 loss:  0.0010187518782913685 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","73 loss:  0.0010043466463685036 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","74 loss:  0.0009905125480145216 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","75 loss:  0.0009771543554961681 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","76 loss:  0.0009642958757467568 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","77 loss:  0.0009518420556560159 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","78 loss:  0.0009398403344675899 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","79 loss:  0.0009282194077968597 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","80 loss:  0.0009169793920591474 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","81 loss:  0.0009060486336238682 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","82 loss:  0.000895427365321666 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","83 loss:  0.0008851868915371597 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","84 loss:  0.000875208352226764 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","85 loss:  0.0008654200355522335 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","86 loss:  0.0008559888228774071 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","87 loss:  0.0008467005682177842 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","88 loss:  0.0008377215708605945 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","89 loss:  0.0008288854733109474 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","90 loss:  0.0008202875033020973 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","91 loss:  0.0008118798723444343 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","92 loss:  0.0008036390063352883 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","93 loss:  0.0007955883629620075 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","94 loss:  0.0007877759635448456 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","95 loss:  0.0007800586754456162 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","96 loss:  0.000772508152294904 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","97 loss:  0.0007650288753211498 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","98 loss:  0.0007578115910291672 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","99 loss:  0.0007506656693294644 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"]}]},{"cell_type":"markdown","metadata":{"id":"bl-WCxLbIfzG"},"source":["## 02. 문자 단위 RNN(Char RNN) - 더 많은 데이터\n"]},{"cell_type":"markdown","metadata":{"id":"FBFOOiFhIhx8"},"source":["이번 챕터에서는 더 많은 데이터 문자 단위 RNN을 구현합니다."]},{"cell_type":"markdown","metadata":{"id":"PYsmXbHhIiKA"},"source":["### 2. 문자 단위 RNN(Char RNN)\n"]},{"cell_type":"markdown","metadata":{"id":"InUKbGfkIjt0"},"source":["우선 필요한 도구들을 임포트합니다."]},{"cell_type":"code","metadata":{"id":"pIWL6r95IlEJ"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aIgh3vzZIoqW"},"source":["다음과 같이 임의의 샘플을 만듭니다."]},{"cell_type":"markdown","metadata":{"id":"nlELwUaVIpam"},"source":["### 1. 훈련 데이터 전처리하기\n"]},{"cell_type":"code","metadata":{"id":"YJ0gQqYZInm7"},"source":["sentence = (\"if you want to build a ship, don't drum up people together to \"\n","            \"collect wood and don't assign them tasks and work, but rather \"\n","            \"teach them to long for the endless immensity of the sea.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cJjwh_HlIs8f"},"source":["문자 집합을 생성하고, 각 문자에 고유한 정수를 부여합니다."]},{"cell_type":"code","metadata":{"id":"bEUfSHRhIuM_"},"source":["char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n","char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lH0kCSKeIvmX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794638242,"user_tz":-540,"elapsed":10,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"43aac140-2ad8-4bb7-ff49-b84b8a87183f"},"source":["print(char_dic) # 공백도 여기서는 하나의 원소"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'a': 0, ' ': 1, 'c': 2, 'i': 3, 'b': 4, \"'\": 5, 'm': 6, 'y': 7, 't': 8, 'n': 9, ',': 10, 'h': 11, 'g': 12, 'o': 13, 'k': 14, 'u': 15, 'p': 16, 'w': 17, 's': 18, '.': 19, 'f': 20, 'e': 21, 'r': 22, 'd': 23, 'l': 24}\n"]}]},{"cell_type":"markdown","metadata":{"id":"sSouE8AiIxYo"},"source":["각 문자에 정수가 부여되었으며, 총 25개의 문자가 존재합니다. 문자 집합의 크기를 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"xJwfTdW6Ix8a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794638242,"user_tz":-540,"elapsed":6,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"1cf409e9-1a78-4adf-ec29-6ac7fbf3c0fd"},"source":["dic_size = len(char_dic)\n","print('문자 집합의 크기 : {}'.format(dic_size))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["문자 집합의 크기 : 25\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZV97EfZaI0Iv"},"source":["문자 집합의 크기는 25이며, 입력을 원-핫 벡터로 사용할 것이므로 이는 매 시점마다 들어갈 입력의 크기이기도 합니다. 이제 하이퍼파라미터를 설정합니다. hidden_size(은닉 상태의 크기)를 입력의 크기와 동일하게 줬는데, 이는 사용자의 선택으로 다른 값을 줘도 무방합니다.\n","\n","그리고 sequence_length라는 변수를 선언했는데, 우리가 앞서 만든 샘플을 10개 단위로 끊어서 샘플을 만들 예정이기 때문입니다. 이는 뒤에서 더 자세히 보겠습니다."]},{"cell_type":"code","metadata":{"id":"ws-XwAiVI1mD"},"source":["# 하이퍼파라미터 설정\n","hidden_size = dic_size\n","sequence_length = 10  # 임의 숫자 지정\n","learning_rate = 0.1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZjuKNyKEI2qL"},"source":["다음은 임의로 지정한 sequence_length 값인 10의 단위로 샘플들을 잘라서 데이터를 만드는 모습을 보여줍니다."]},{"cell_type":"code","metadata":{"id":"oLXi1Mz_I3_l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794638526,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"c52ee626-6a4b-45f1-805f-f827231f3951"},"source":["# 데이터 구성\n","x_data = []\n","y_data = []\n","\n","for i in range(0, len(sentence) - sequence_length):\n","    x_str = sentence[i:i + sequence_length]\n","    y_str = sentence[i + 1: i + sequence_length + 1]\n","    print(i, x_str, '->', y_str)\n","\n","    x_data.append([char_dic[c] for c in x_str])  # x str to index\n","    y_data.append([char_dic[c] for c in y_str])  # y str to index"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 if you wan -> f you want\n","1 f you want ->  you want \n","2  you want  -> you want t\n","3 you want t -> ou want to\n","4 ou want to -> u want to \n","5 u want to  ->  want to b\n","6  want to b -> want to bu\n","7 want to bu -> ant to bui\n","8 ant to bui -> nt to buil\n","9 nt to buil -> t to build\n","10 t to build ->  to build \n","11  to build  -> to build a\n","12 to build a -> o build a \n","13 o build a  ->  build a s\n","14  build a s -> build a sh\n","15 build a sh -> uild a shi\n","16 uild a shi -> ild a ship\n","17 ild a ship -> ld a ship,\n","18 ld a ship, -> d a ship, \n","19 d a ship,  ->  a ship, d\n","20  a ship, d -> a ship, do\n","21 a ship, do ->  ship, don\n","22  ship, don -> ship, don'\n","23 ship, don' -> hip, don't\n","24 hip, don't -> ip, don't \n","25 ip, don't  -> p, don't d\n","26 p, don't d -> , don't dr\n","27 , don't dr ->  don't dru\n","28  don't dru -> don't drum\n","29 don't drum -> on't drum \n","30 on't drum  -> n't drum u\n","31 n't drum u -> 't drum up\n","32 't drum up -> t drum up \n","33 t drum up  ->  drum up p\n","34  drum up p -> drum up pe\n","35 drum up pe -> rum up peo\n","36 rum up peo -> um up peop\n","37 um up peop -> m up peopl\n","38 m up peopl ->  up people\n","39  up people -> up people \n","40 up people  -> p people t\n","41 p people t ->  people to\n","42  people to -> people tog\n","43 people tog -> eople toge\n","44 eople toge -> ople toget\n","45 ople toget -> ple togeth\n","46 ple togeth -> le togethe\n","47 le togethe -> e together\n","48 e together ->  together \n","49  together  -> together t\n","50 together t -> ogether to\n","51 ogether to -> gether to \n","52 gether to  -> ether to c\n","53 ether to c -> ther to co\n","54 ther to co -> her to col\n","55 her to col -> er to coll\n","56 er to coll -> r to colle\n","57 r to colle ->  to collec\n","58  to collec -> to collect\n","59 to collect -> o collect \n","60 o collect  ->  collect w\n","61  collect w -> collect wo\n","62 collect wo -> ollect woo\n","63 ollect woo -> llect wood\n","64 llect wood -> lect wood \n","65 lect wood  -> ect wood a\n","66 ect wood a -> ct wood an\n","67 ct wood an -> t wood and\n","68 t wood and ->  wood and \n","69  wood and  -> wood and d\n","70 wood and d -> ood and do\n","71 ood and do -> od and don\n","72 od and don -> d and don'\n","73 d and don' ->  and don't\n","74  and don't -> and don't \n","75 and don't  -> nd don't a\n","76 nd don't a -> d don't as\n","77 d don't as ->  don't ass\n","78  don't ass -> don't assi\n","79 don't assi -> on't assig\n","80 on't assig -> n't assign\n","81 n't assign -> 't assign \n","82 't assign  -> t assign t\n","83 t assign t ->  assign th\n","84  assign th -> assign the\n","85 assign the -> ssign them\n","86 ssign them -> sign them \n","87 sign them  -> ign them t\n","88 ign them t -> gn them ta\n","89 gn them ta -> n them tas\n","90 n them tas ->  them task\n","91  them task -> them tasks\n","92 them tasks -> hem tasks \n","93 hem tasks  -> em tasks a\n","94 em tasks a -> m tasks an\n","95 m tasks an ->  tasks and\n","96  tasks and -> tasks and \n","97 tasks and  -> asks and w\n","98 asks and w -> sks and wo\n","99 sks and wo -> ks and wor\n","100 ks and wor -> s and work\n","101 s and work ->  and work,\n","102  and work, -> and work, \n","103 and work,  -> nd work, b\n","104 nd work, b -> d work, bu\n","105 d work, bu ->  work, but\n","106  work, but -> work, but \n","107 work, but  -> ork, but r\n","108 ork, but r -> rk, but ra\n","109 rk, but ra -> k, but rat\n","110 k, but rat -> , but rath\n","111 , but rath ->  but rathe\n","112  but rathe -> but rather\n","113 but rather -> ut rather \n","114 ut rather  -> t rather t\n","115 t rather t ->  rather te\n","116  rather te -> rather tea\n","117 rather tea -> ather teac\n","118 ather teac -> ther teach\n","119 ther teach -> her teach \n","120 her teach  -> er teach t\n","121 er teach t -> r teach th\n","122 r teach th ->  teach the\n","123  teach the -> teach them\n","124 teach them -> each them \n","125 each them  -> ach them t\n","126 ach them t -> ch them to\n","127 ch them to -> h them to \n","128 h them to  ->  them to l\n","129  them to l -> them to lo\n","130 them to lo -> hem to lon\n","131 hem to lon -> em to long\n","132 em to long -> m to long \n","133 m to long  ->  to long f\n","134  to long f -> to long fo\n","135 to long fo -> o long for\n","136 o long for ->  long for \n","137  long for  -> long for t\n","138 long for t -> ong for th\n","139 ong for th -> ng for the\n","140 ng for the -> g for the \n","141 g for the  ->  for the e\n","142  for the e -> for the en\n","143 for the en -> or the end\n","144 or the end -> r the endl\n","145 r the endl ->  the endle\n","146  the endle -> the endles\n","147 the endles -> he endless\n","148 he endless -> e endless \n","149 e endless  ->  endless i\n","150  endless i -> endless im\n","151 endless im -> ndless imm\n","152 ndless imm -> dless imme\n","153 dless imme -> less immen\n","154 less immen -> ess immens\n","155 ess immens -> ss immensi\n","156 ss immensi -> s immensit\n","157 s immensit ->  immensity\n","158  immensity -> immensity \n","159 immensity  -> mmensity o\n","160 mmensity o -> mensity of\n","161 mensity of -> ensity of \n","162 ensity of  -> nsity of t\n","163 nsity of t -> sity of th\n","164 sity of th -> ity of the\n","165 ity of the -> ty of the \n","166 ty of the  -> y of the s\n","167 y of the s ->  of the se\n","168  of the se -> of the sea\n","169 of the sea -> f the sea.\n"]}]},{"cell_type":"markdown","metadata":{"id":"9t6cV1ipI5Lz"},"source":["총 170개의 샘플이 생성되었습니다. 그리고 각 샘플의 각 문자들은 고유한 정수로 인코딩이 된 상태입니다. 첫번째 샘플의 입력 데이터와 레이블 데이터를 출력해봅시다."]},{"cell_type":"code","metadata":{"id":"70rB95ypI6UX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794638527,"user_tz":-540,"elapsed":10,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"fae065a1-60a6-4ec5-da77-aff03d6e01c2"},"source":["print(x_data[0])\n","print(y_data[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[3, 20, 1, 7, 13, 15, 1, 17, 0, 9]\n","[20, 1, 7, 13, 15, 1, 17, 0, 9, 8]\n"]}]},{"cell_type":"markdown","metadata":{"id":"xnOfi4JLI77I"},"source":["한 칸씩 쉬프트 된 시퀀스가 정상적으로 출력되는 것을 볼 수 있습니다. 이제 입력 시퀀스에 대해서 원-핫 인코딩을 수행하고, 입력 데이터와 레이블 데이터를 텐서로 변환합니다."]},{"cell_type":"code","metadata":{"id":"-H861YS8I6Ya"},"source":["x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n","X = torch.FloatTensor(x_one_hot)\n","Y = torch.LongTensor(y_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZwcQlU6OI996"},"source":["이제 훈련 데이터와 레이블 데이터의 크기를 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"0Wv8IFWDI_Ro","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794638528,"user_tz":-540,"elapsed":10,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"1e1f7eb5-f138-4802-f9cf-bfba527b0a8b"},"source":["print('훈련 데이터의 크기 : {}'.format(X.shape))\n","print('레이블의 크기 : {}'.format(Y.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 데이터의 크기 : torch.Size([170, 10, 25])\n","레이블의 크기 : torch.Size([170, 10])\n"]}]},{"cell_type":"markdown","metadata":{"id":"RDng_R_vJAkt"},"source":["원-핫 인코딩 된 결과를 보기 위해서 첫번째 샘플만 출력해봅시다."]},{"cell_type":"code","metadata":{"id":"qShtnSQ8JBtN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794638528,"user_tz":-540,"elapsed":9,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"e805f6ea-dc0d-4c6d-956f-a8809513de84"},"source":["print(X[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 1., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0.]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"mNxBH8NnJEbG"},"source":["레이블 데이터의 첫번째 샘플도 출력해봅시다."]},{"cell_type":"code","metadata":{"id":"TmAMME5lJGSJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794638528,"user_tz":-540,"elapsed":8,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"e8a7728b-70ad-497f-a813-49781b7de71a"},"source":["print(Y[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([20,  1,  7, 13, 15,  1, 17,  0,  9,  8])\n"]}]},{"cell_type":"markdown","metadata":{"id":"31zs8JjSJHW-"},"source":["### 2. 모델 구현하기\n"]},{"cell_type":"markdown","metadata":{"id":"rKyrdSkrJJEF"},"source":["모델은 앞서 실습한 문자 단위 RNN 챕터와 거의 동일합니다. 다만 이번에는 은닉층을 두 개 쌓을 겁니다."]},{"cell_type":"code","metadata":{"id":"2BEdUCUcJKST"},"source":["class Net(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n","        super(Net, self).__init__()\n","        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n","        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n","\n","    def forward(self, x):\n","        x, _status = self.rnn(x)\n","        x = self.fc(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gZo4CLyLJK3O"},"source":["net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V2ji_7wRJNnd"},"source":["nn.RNN() 안에 num_layers라는 인자를 사용합니다. 이는 은닉층을 몇 개 쌓을 것인지를 의미합니다. 모델 선언 시 layers라는 인자에 2를 전달하여 은닉층을 두 개 쌓습니다. 비용 함수와 옵티마이저를 선언합니다."]},{"cell_type":"code","metadata":{"id":"OyQHDmjTJPDy"},"source":["criterion = torch.nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AJlsSbmoJP9j"},"source":["이제 모델에 입력을 넣어서 출력의 크기를 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"eZYI999JJM3Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794638921,"user_tz":-540,"elapsed":400,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"87b598d1-408d-4210-dd54-f3d63f95f46e"},"source":["outputs = net(X)\n","print(outputs.shape) # 3차원 텐서"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([170, 10, 25])\n"]}]},{"cell_type":"markdown","metadata":{"id":"9Dk1_STwJTdp"},"source":["(170, 10, 25)의 크기를 가지는데 각각 배치 차원, 시점(timesteps), 출력의 크기입니다. 나중에 정확도를 측정할 때는 이를 모두 펼쳐서 계산하게 되는데, 이때는 view를 사용하여 배치 차원과 시점 차원을 하나로 만듭니다."]},{"cell_type":"code","metadata":{"id":"2koPYnfwJSVP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794638921,"user_tz":-540,"elapsed":7,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"d617c535-5221-4cbf-9cd1-cd62593bb88c"},"source":["print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1700, 25])\n"]}]},{"cell_type":"markdown","metadata":{"id":"bf5cZlh8JVhL"},"source":["차원이 (1700, 25)가 된 것을 볼 수 있습니다. 이제 레이블 데이터의 크기를 다시 복습봅시다."]},{"cell_type":"code","metadata":{"id":"0pJy9v1TJWrk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794638921,"user_tz":-540,"elapsed":6,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"aaf4c6c5-f4ec-401b-fe41-da40ebc7c482"},"source":["print(Y.shape)\n","print(Y.view(-1).shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([170, 10])\n","torch.Size([1700])\n"]}]},{"cell_type":"markdown","metadata":{"id":"fwYedIupJYJb"},"source":["레이블 데이터는 (170, 10)의 크기를 가지는데, 마찬가지로 나중에 정확도를 측정할 때는 이걸 펼쳐서 계산할 예정입니다. 이 경우 (1700)의 크기를 가지게 됩니다. 이제 옵티마이저와 손실 함수를 정의합니다."]},{"cell_type":"code","metadata":{"id":"RYNrO5odJZfB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794640227,"user_tz":-540,"elapsed":1311,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"ddadf1e5-24a6-48ad-cd5b-1d91ee8a1ff0"},"source":["for i in range(100):\n","    optimizer.zero_grad()\n","    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n","    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n","    loss.backward()\n","    optimizer.step()\n","\n","    # results의 텐서 크기는 (170, 10)\n","    results = outputs.argmax(dim=2)\n","    predict_str = \"\"\n","    for j, result in enumerate(results):\n","        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n","            predict_str += ''.join([char_set[t] for t in result])\n","        else: # 그 다음에는 마지막 글자만 반복 추가\n","            predict_str += char_set[result[-1]]\n","\n","    print(predict_str)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["kummmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\n","e                                                                                                                                                                                  \n","ttt ttt ttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt\n","                                                                                                                                                                                   \n","m .....k. h hhohh h hlhhehoh hhhohoho hlhsllhhhh the o hhhhhhthh ohhh hh   sh thh ohehohhhhlh ohohe hh hhhohhaohthe thhhho hhh h  hhh  hh hhhhohh hhh thhhh sho i hhhehhhh  hhhhhhl\n","r .tililmrrrrrr rprrrrrrrrrrr mrrrrrrrrmurmrrrrrrr rrmrrrrrrrrrrrrrrrrrrrrrmrrrrmrr mrmrrmrrrrrrr rrlrrrrrrrr r glmrrrr rrrrrrrurrrrrrrrrrrrrrrrrrrrmlmrrrrrmrm mlrrrrrrrrrrrrrrrrr\n","t woiop dt   dtdd,n   tt dd to,   d dd a,    dd  ttd d ddd    n  dd  dtd  dd  d dtdolr ddop  an dod,  tdd t dtdoar  d dtdtnt ttr ddd  dp  r d d   t ur  dtd  dtok,d td dd dtndd dt \n","towok  owkotoewkk  dno...kkocen ookwoboe tteokwokwokwknooeno.k dokk oykk ww.o wo.aw.ttokk  wo  twkkn twocet rkket wewkkkwkntoket tkk t  dw dwow w woe t wkm dorketowkco.ko'.t'ktoh \n","tewentooe o h s .t 'tos t eod t oesh.te  't hseh shtes ehtto    oen o    oe  otot h tto e toh  owhet  okr t e s t t s e h  toe  ooe  h  sw sd k k to  t h e tos   oh s   ok  e to  \n","lete  oe th e t et  teeotc otot'oeehete o'  e ce tone eeet h c  d e e o  oent tee'e t sts toee  th  t hee tot't t t tot ec  o c edoe ecoe o d t e teectoh t'tetu  oete  toet e te  \n","loto  oec hto bot'o totetco t t tec tbncontoec e t  e tet te lh d a elo  htot tot'tet set 'tee  'os'tmten tot'r t t tonhec thec  t eottoeco t tot'teectos a t to  btple to toectmsl\n","loto  ta  aao totkd tototto tot theo ' to toe  e pheo ten th th e co to  saps tot'tetos t  toem tos thtnd toe't tot t shem th p et  r th to p tot thet ha oe  to  mt t  to toerthal\n","loto  to t ao toskd thtndlo totpthtos  an to   e themster to thnntmo to  sam  ton t  ss t  to m to tthtnd ton t aor tont e to gt t  g to to g tos to mshs e s ao  m  to to toemth s\n","l tm  to t ao tt lo tntoel, tor t tos  tosto     th e  er t  ton er  ta   ay  ton t tn  t  to r to t  tnt tonlt ans a t  m to gt t er to to g tnr toerton e s an  ms to to toemt es\n","l emr to t aonbs lo tnt ep, t elt eusist,stu m e to ep er t  ton   t aon  a   ton t to  m  toer to ts dnt ton , anr r t er to p, roer to toer t r toerton e s am  es t  to toert  g\n","l tmr toct ao tusle tsthep, don'thto m t, bo m e toeem er t rto   m, ao t an  ton t tosim  toer to ps dnt won', dnt d t em t  p, doem toeto m tormtoemt r e s an  ns d  to doemt ag\n","lodmr toct dontudle tsthec, don't do t t,slo t e toeemher theto t rt aonc and tos't anslm  toem toip, dnt won', aot d t em th c, toem toeto c torcthemtod e t am ens do to aoemt rl\n","lhdmr tont aontusle tsthec, don't dos'e', lo c e tonemher theco tech aong and toe't dnsigh toem tosl, dnd won', aut dmthem th ch doer tonco c torlthettos ent am ens ie to themthrc\n","lhamm wont aontusle tsthecf don'd dos' t, lo ele to emher theco teth do g and toe'dhansig  them to ps dnd aon', amt dmthem th ch doem tonco g torcthemtos ent am enshie ao themthac\n","lhaem wont ao tusle tsthepf don'd dosm t, lo e e theemher theto leth do t and tos'd ansig  them todps and wonl, dmt dot em thich dhem to to d torcthe tos enu am enshie an themtoa'\n","lhaor wont ao tusle tsthep, don'd doso  , lo   e thgerher th to l dt to g and ton'd ansign them tosls and wonl, ant dot em thich them to co d tor the tod ens am ens uo an thepsnds\n","l aor wont ao cusld tsthip, don't doume , beo le togerher thrco l ct wond and ton't dnsign toem todls ant wonk, but rot em thich toem torco d for the  odle s wm ens io an thepsad'\n","l tor wont ao tuild anthip, don't doum kp teo  e to ether th co   ct wond and ton't dnsign toem tosks and wonk, but rothem thich ther to co g for the  odle u wm ens i  af toeps d'\n","l tot wont to tuild anthip, ton't doum cp teo le together th co l ct wond and ton't ansign toer tosks and wonk, aut rother th ch ther to co g tor the tosle u wm ens i  af toepsndc\n","l tot wont to tuild anthip, don't rrum tp peo le together th co lect wood and ton't assign thep tosks and wonk, aut rother thgch thep torcong tor the tonle s wm ensig  af thepsarc\n","l tot want to tuild asship, don't drum lp peo le together th co lect wood and ton't assign thep tosks and wonk, aut rother thgch them torcong tor the tosle s wm ensig  af thepsarc\n","l tot want do build anship, don't arum up peo le thgether th co lect wood and won't assign them tosks and work, dut rother thach ther to cong tor the tasle s im ensit  af themtedc\n","l tot want do build anship, don't arum up beonle thgether th co lect wood and won't assign them tosks and work, but rother thach them to cong tor the  osless im ensit  af thems dc\n","l tot wont do build anship, don't arum up beonle together th co lect wood and don't assign them tosks and dork, but rother thach them to long foo the  oske s im ens ty af them  d'\n","l tot want do build anship, don't arum up peotle together th co lect wood and don't assign them tosks and dork, but rother thach them to long for the toske s im ens ty of themt d'\n","g ton want do build anship, don't drum up peotle together th co lect wood and don't assign them tosks and dork, but rother thach them to long for the tns e s im ensity of themtedc\n","g ton want do build anship, don't arum up people together th co lect wood and don't assign the  tosks and dork, but rother thach them to long for the tns e s im ensity of the fnd'\n","m eon want do build asship, don't drum up people together th co lect wood and don't assign them tosks and work, but rother teach them to long for the end e s immensity of themsaac\n","m eot want to build asship, don't drum up people together th co lect wood and won'i assign them tosks and work, but rather teach them to long for the endle s immensity of themsnac\n","m eot want to build asship, don't drum up people together te co lect wood and won't assign them tosks and work, but rather toach the  to long for thependless immensity of the saac\n","m eon want to build asship, don't arum up people together to co lect wood and don't assign them tosks and work, but rather toach them to long for thependless immensity of the saac\n","m eon want to build asship, don't drum up people together to co lect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the saa'\n","m eou want to build anship, don't drum up people together to collect wood and don't dssign them tasks and dork, but rather toach them to long for the endless immensity of the saa'\n","l eou want to build anship, don't drum up people together to co lect wood and don't assign them tasks and dork, but rather toach them to long for the endless immensity of the saa'\n","l eou want to build anship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea'\n","l eou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea'\n","g eou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the seal\n","g eou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the seal\n","g tou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea'\n","g tou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the seak\n","g tou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seak\n","p tou want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seak\n","l tou want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seak\n","l eou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seak\n","l eou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seak\n","l eou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the seak\n","l eou want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seak\n","g eou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seak\n","g eou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea'\n","g eou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n","g eou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n","g eou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seak\n","p eou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea \n","p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea \n","t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea \n","f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n","m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n","t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n","m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n","l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n","l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n","m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n","m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n","l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n","l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n","l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n","t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n","t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n","t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"]}]},{"cell_type":"markdown","metadata":{"id":"-55Cmg1EJbDO"},"source":["처음에는 이상한 예측을 하지만 마지막 에포크에서는 꽤 정확한 문자을 생성하는 것을 볼 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"ZtDNN469Jx5g"},"source":["# 03. 단어 단위 RNN - 임베딩 사용"]},{"cell_type":"markdown","metadata":{"id":"bH1BLyF8J3Z6"},"source":["이번 챕터에서는 문자 단위가 아니라 RNN의 입력 단위를 단어 단위로 사용합니다. 그리고 단어 단위를 사용함에 따라서 Pytorch에서 제공하는 임베딩 층(embedding layer)를 사용하겠습니다."]},{"cell_type":"markdown","metadata":{"id":"SpD9nC8EJ5Ba"},"source":["### 1. 훈련 데이터 전처리하기\n"]},{"cell_type":"markdown","metadata":{"id":"IffdcQq9J6eb"},"source":["우선 실습을 위해 필요한 도구들을 임포트합니다."]},{"cell_type":"code","metadata":{"id":"5RD70bgKJ67V"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H1liOjglJ8Mx"},"source":["실습을 위해 임의의 문장을 만듭니다."]},{"cell_type":"code","metadata":{"id":"orNpsH_aJ-AS"},"source":["sentence = \"Repeat is the best medicine for memory\".split()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ct7tyjPJ_I8"},"source":["우리가 만들 RNN은 'Repeat is the best medicine for'을 입력받으면 'is the best medicine for memory'를 출력하는 RNN입니다. 위의 임의의 문장으로부터 단어장(vocabulary)을 만듭니다."]},{"cell_type":"code","metadata":{"id":"1r9BIrxpKAV5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794640229,"user_tz":-540,"elapsed":7,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"cb2e9d3c-cf10-4920-c700-b6cd57834c49"},"source":["vocab = list(set(sentence))\n","print(vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['medicine', 'Repeat', 'best', 'for', 'is', 'memory', 'the']\n"]}]},{"cell_type":"markdown","metadata":{"id":"QXCc6aQyKBpL"},"source":["이제 단어장의 단어에 고유한 정수 인덱스를 부여합니다. 그리고 그와 동시에 모르는 단어를 의미하는 UNK 토큰도 추가하겠습니다."]},{"cell_type":"code","metadata":{"id":"25cKmWS8KBhX"},"source":["word2index = {tkn: i for i, tkn in enumerate(vocab, 1)}  # 단어에 고유한 정수 부여\n","word2index['<unk>']=0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"waTq8t1IKGU3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794640229,"user_tz":-540,"elapsed":6,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"7afa2641-d72c-4350-99e0-7f29ba5adb35"},"source":["print(word2index)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'medicine': 1, 'Repeat': 2, 'best': 3, 'for': 4, 'is': 5, 'memory': 6, 'the': 7, '<unk>': 0}\n"]}]},{"cell_type":"markdown","metadata":{"id":"LV-enmMLKH6H"},"source":["이제 word2index가 우리가 사용할 최종 단어장인 셈입니다. word2index에 단어를 입력하면 맵핑되는 정수를 리턴합니다."]},{"cell_type":"code","metadata":{"id":"XuCOesMlKJAz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794640609,"user_tz":-540,"elapsed":385,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"c83bc033-68d2-48f1-a58a-37045567acba"},"source":["print(word2index['memory'])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6\n"]}]},{"cell_type":"markdown","metadata":{"id":"Nlnfuc2iKKJJ"},"source":["단어 'memory'와 맵핑되는 정수는 2입니다. 예측 단계에서 예측한 문장을 확인하기 위해 idx2word도 만듭니다."]},{"cell_type":"code","metadata":{"id":"Mj5m7Sl3KMaI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794640609,"user_tz":-540,"elapsed":8,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"d35d2e17-d1a8-4faa-c7d2-727480d54f6d"},"source":["# 수치화된 데이터를 단어로 바꾸기 위한 사전\n","index2word = {v: k for k, v in word2index.items()}\n","print(index2word)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{1: 'medicine', 2: 'Repeat', 3: 'best', 4: 'for', 5: 'is', 6: 'memory', 7: 'the', 0: '<unk>'}\n"]}]},{"cell_type":"markdown","metadata":{"id":"f_E8nmtGKKOV"},"source":["idx2word는 정수로부터 단어를 리턴하는 역할을 합니다. 정수 2를 넣어봅시다."]},{"cell_type":"code","metadata":{"id":"9DWTTlWXKOb7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794640609,"user_tz":-540,"elapsed":7,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"ed5ed088-4ad8-44a2-b4e1-2a5eedda4935"},"source":["print(index2word[2])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Repeat\n"]}]},{"cell_type":"markdown","metadata":{"id":"Y3Zsp0wFKPg4"},"source":["정수 2와 맵핑되는 단어는 memory인 것을 확인할 수 있습니다. 이제 데이터의 각 단어를 정수로 인코딩하는 동시에, 입력 데이터와 레이블 데이터를 만드는 build_data라는 함수를 만들어보겠습니다."]},{"cell_type":"code","metadata":{"id":"kDxlolmFKQBq"},"source":["def build_data(sentence, word2index):\n","    encoded = [word2index[token] for token in sentence] # 각 문자를 정수로 변환. \n","    input_seq, label_seq = encoded[:-1], encoded[1:] # 입력 시퀀스와 레이블 시퀀스를 분리\n","    input_seq = torch.LongTensor(input_seq).unsqueeze(0) # 배치 차원 추가\n","    label_seq = torch.LongTensor(label_seq).unsqueeze(0) # 배치 차원 추가\n","    return input_seq, label_seq"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TWxtOXhrKRe_"},"source":["만들어진 함수로부터 입력 데이터와 레이블 데이터를 얻습니다."]},{"cell_type":"code","metadata":{"id":"ulUZDdB_KStH"},"source":["X, Y = build_data(sentence, word2index)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H4Okkud2KT9a"},"source":["입력 데이터와 레이블 데이터가 정상적으로 생성되었는지 출력해봅시다."]},{"cell_type":"code","metadata":{"id":"GhDA01imKXOL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794640611,"user_tz":-540,"elapsed":8,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"a2fde092-bbf3-4374-ce3e-962d5ff1f99a"},"source":["print(X)\n","print(Y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[2, 5, 7, 3, 1, 4]])\n","tensor([[5, 7, 3, 1, 4, 6]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"l9uSO3XjKYcG"},"source":["### 2. 모델 구현하기"]},{"cell_type":"markdown","metadata":{"id":"iZumEueXKbVg"},"source":["이제 모델을 설계합니다. 이전 모델들과 달라진 점은 임베딩 층을 추가했다는 점입니다. 파이토치에서는 nn.Embedding()을 사용해서 임베딩 층을 구현합니다. 임베딩층은 크게 두 가지 인자를 받는데 첫번째 인자는 단어장의 크기이며, 두번째 인자는 임베딩 벡터의 차원입니다."]},{"cell_type":"code","metadata":{"id":"gMeHajNGKc-8"},"source":["class Net(nn.Module):\n","    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n","        super(Net, self).__init__()\n","        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, # 워드 임베딩\n","                                            embedding_dim=input_size)\n","        self.rnn_layer = nn.RNN(input_size, hidden_size, # 입력 차원, 은닉 상태의 크기 정의\n","                                batch_first=batch_first)\n","        self.linear = nn.Linear(hidden_size, vocab_size) # 출력은 원-핫 벡터의 크기를 가져야함. 또는 단어 집합의 크기만큼 가져야함.\n","\n","    def forward(self, x):\n","        # 1. 임베딩 층\n","        # 크기변화: (배치 크기, 시퀀스 길이) => (배치 크기, 시퀀스 길이, 임베딩 차원)\n","        output = self.embedding_layer(x)\n","        # 2. RNN 층\n","        # 크기변화: (배치 크기, 시퀀스 길이, 임베딩 차원)\n","        # => output (배치 크기, 시퀀스 길이, 은닉층 크기), hidden (1, 배치 크기, 은닉층 크기)\n","        output, hidden = self.rnn_layer(output)\n","        # 3. 최종 출력층\n","        # 크기변화: (배치 크기, 시퀀스 길이, 은닉층 크기) => (배치 크기, 시퀀스 길이, 단어장 크기)\n","        output = self.linear(output)\n","        # 4. view를 통해서 배치 차원 제거\n","        # 크기변화: (배치 크기, 시퀀스 길이, 단어장 크기) => (배치 크기*시퀀스 길이, 단어장 크기)\n","        return output.view(-1, output.size(2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dEQK5Gl5KeUl"},"source":["이제 모델을 위해 하이퍼파라미터를 설정합니다."]},{"cell_type":"code","metadata":{"id":"DH-scZD4Kf9w"},"source":["# 하이퍼 파라미터\n","vocab_size = len(word2index)  # 단어장의 크기는 임베딩 층, 최종 출력층에 사용된다. <unk> 토큰을 크기에 포함한다.\n","input_size = 5  # 임베딩 된 차원의 크기 및 RNN 층 입력 차원의 크기\n","hidden_size = 20  # RNN의 은닉층 크기"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Ib-vc4eKhAc"},"source":["모델을 생성합니다.\n"]},{"cell_type":"code","metadata":{"id":"YZD1tcalKiVd"},"source":["# 모델 생성\n","model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n","# 손실함수 정의\n","loss_function = nn.CrossEntropyLoss() # 소프트맥스 함수 포함이며 실제값은 원-핫 인코딩 안 해도 됨.\n","# 옵티마이저 정의\n","optimizer = optim.Adam(params=model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otvPGNKoKjgP"},"source":["모델에 입력을 넣어서 출력을 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"x6oHlD1CKkFh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794640612,"user_tz":-540,"elapsed":8,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"840efb81-b383-405b-fc30-89acba7d4d0c"},"source":["# 임의로 예측해보기. 가중치는 전부 랜덤 초기화 된 상태이다.\n","output = model(X)\n","print(output)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.1611, -0.0612, -0.1192, -0.1996, -0.1597,  0.1927,  0.1063,  0.1457],\n","        [-0.4126, -0.2364, -0.2545, -0.5599, -0.2718, -0.1986,  0.1018, -0.0931],\n","        [-0.1189, -0.2396, -0.1202, -0.6827, -0.2776, -0.4387,  0.1492,  0.1545],\n","        [ 0.1076, -0.0316, -0.2023, -0.3697, -0.1470, -0.2619, -0.0283,  0.1793],\n","        [ 0.3077, -0.0206, -0.3697, -0.0577,  0.0934,  0.0957,  0.1668,  0.5142],\n","        [-0.2550,  0.0404, -0.2842, -0.2434, -0.0866,  0.0633,  0.1435,  0.1039]],\n","       grad_fn=<ViewBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"bcLqWmkLKmWH"},"source":["모델이 어떤 예측값을 내놓기는 하지만 현재 가중치는 랜덤 초기화되어 있어 의미있는 예측값은 아닙니다. 예측값의 크기를 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"KQg8PyMcKnEr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794640612,"user_tz":-540,"elapsed":8,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"41afcd19-5f51-4fdf-cd63-13b0800ba1f4"},"source":["print(output.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([6, 8])\n"]}]},{"cell_type":"markdown","metadata":{"id":"ylpPMAMvKoTD"},"source":["예측값의 크기는 (6, 8)입니다. 이는 각각 (시퀀스의 길이, 은닉층의 크기)에 해당됩니다. 모델은 훈련시키기 전에 예측을 제대로 하고 있는지 예측된 정수 시퀀스를 다시 단어 시퀀스로 바꾸는 decode 함수를 만듭니다."]},{"cell_type":"code","metadata":{"id":"euAl0rfJKpxD"},"source":["# 수치화된 데이터를 단어로 전환하는 함수\n","decode = lambda y: [index2word.get(x) for x in y]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mVFDDrv6KryV"},"source":["약 200 에포크 학습합니다."]},{"cell_type":"code","metadata":{"id":"lDu4YOx2Kssp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794641121,"user_tz":-540,"elapsed":516,"user":{"displayName":"Hyun-Chul Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF9b8NdYi-SRUg7E_yt_owwxDd3ZKMBuC4cAZS=s64","userId":"14185916921863952694"}},"outputId":"f66e020c-1778-4445-e411-2752ab034421"},"source":["# 훈련 시작\n","for step in range(201):\n","    # 경사 초기화\n","    optimizer.zero_grad()\n","    # 순방향 전파\n","    output = model(X)\n","    # 손실값 계산\n","    loss = loss_function(output, Y.view(-1))\n","    # 역방향 전파\n","    loss.backward()\n","    # 매개변수 업데이트\n","    optimizer.step()\n","    # 기록\n","    if step % 40 == 0:\n","        print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\n","        pred = output.softmax(-1).argmax(-1).tolist()\n","        print(\" \".join([\"Repeat\"] + decode(pred)))\n","        print()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[01/201] 2.0796 \n","Repeat is memory the the the memory\n","\n","[41/201] 1.4936 \n","Repeat is the the medicine for memory\n","\n","[81/201] 0.8560 \n","Repeat is the best medicine for memory\n","\n","[121/201] 0.4176 \n","Repeat is the best medicine for memory\n","\n","[161/201] 0.2118 \n","Repeat is the best medicine for memory\n","\n","[201/201] 0.1262 \n","Repeat is the best medicine for memory\n","\n"]}]}]}