{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1VqtUYVX8YSmfACYmyUnv_eLIl1eNTY00","timestamp":1652519198632}],"authorship_tag":"ABX9TyNuu4dx3Kat2kUQDGRUO5z/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZhoMnn88Hp6R"},"source":["참고문헌: PyTorch로 시작하는 딥 러닝 입문, 파이썬 딥러닝 파이토치 (이경택, 방성수, 안상준 지음), 정보문화사"]},{"cell_type":"markdown","metadata":{"id":"8EK4VR7DHbR5"},"source":["# 11. 다대다 RNN을 이용한 텍스트 생성\n"]},{"cell_type":"markdown","metadata":{"id":"20s4gsQjHdyv"},"source":["## 01. 문자 단위 RNN(Char RNN)"]},{"cell_type":"markdown","metadata":{"id":"cMgXNGfeHhK9"},"source":["이번 챕터에서는 모든 시점의 입력에 대해서 모든 시점에 대해서 출력을 하는 다대다 RNN을 구현해봅시다. 다대다 RNN은 대표적으로 품사 태깅, 개체명 인식 등에서 사용됩니다."]},{"cell_type":"markdown","metadata":{"id":"v0AndLccHiaO"},"source":["## 1. 문자 단위 RNN(Char RNN)\n"]},{"cell_type":"markdown","metadata":{"id":"KTFCs8K3Hjna"},"source":["RNN의 입출력의 단위가 단어 레벨(word-level)이 아니라 문자 레벨(character-level)로 하여 RNN을 구현한다면, 이를 문자 단위 RNN이라고 합니다. RNN 구조 자체가 달라진 것은 아니고, 입, 출력의 단위가 문자로 바뀌었을 뿐입니다. 문자 단위 RNN을 다대다 구조로 구현해봅시다."]},{"cell_type":"markdown","metadata":{"id":"kfXLrmQ5Hk1Y"},"source":["우선 필요한 도구들을 임포트합니다."]},{"cell_type":"code","metadata":{"id":"8bm0auFdHmFM","executionInfo":{"status":"ok","timestamp":1677073181854,"user_tz":-540,"elapsed":2930,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QJNf_M11Hr99"},"source":["## 1. 훈련 데이터 전처리하기"]},{"cell_type":"markdown","metadata":{"id":"pUhg6zkJHtlc"},"source":["여기서는 문자 시퀀스 apple을 입력받으면 pple!를 출력하는 RNN을 구현해볼 겁니다. 이렇게 구현하는 어떤 의미가 있지는 않습니다. 그저 RNN의 동작을 이해하기 위한 목적입니다.\n","\n","입력 데이터와 레이블 데이터에 대해서 문자 집합(voabulary)을 만듭니다. 여기서 문자 집합은 중복을 제거한 문자들의 집합입니다."]},{"cell_type":"code","metadata":{"id":"qRVP-CLEHvXw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073181855,"user_tz":-540,"elapsed":21,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"df755980-d05b-4484-9f57-81078007ba8c"},"source":["input_str = 'apple'\n","label_str = 'pple!'\n","char_vocab = sorted(list(set(input_str+label_str)))\n","vocab_size = len(char_vocab)\n","print ('문자 집합의 크기 : {}'.format(vocab_size))"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["문자 집합의 크기 : 5\n"]}]},{"cell_type":"markdown","metadata":{"id":"cmZjVs7AHwfd"},"source":["현재 문자 집합에는 총 5개의 문자가 있습니다. !, a, e, l, p입니다. 이제 하이퍼파라미터를 정의해줍니다. 이때 입력은 원-핫 벡터를 사용할 것이므로 입력의 크기는 문자 집합의 크기여야만 합니다."]},{"cell_type":"code","metadata":{"id":"V_U4VTtGHxts","executionInfo":{"status":"ok","timestamp":1677073181855,"user_tz":-540,"elapsed":18,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n","hidden_size = 5\n","output_size = 5\n","learning_rate = 0.1"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NZYIwaNaHzP7"},"source":["이제 문자 집합에 고유한 정수를 부여합니다."]},{"cell_type":"code","metadata":{"id":"5Nk19M77H1rL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073181856,"user_tz":-540,"elapsed":18,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"e0df3483-9d6d-4914-b032-859d12c2eadf"},"source":["char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n","print(char_to_index)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"]}]},{"cell_type":"markdown","metadata":{"id":"gmIzaPBJH23J"},"source":["!은 0, a는 1, e는 2, l은 3, p는 4가 부여되었습니다. 나중에 예측 결과를 다시 문자 시퀀스로 보기위해서 반대로 정수로부터 문자를 얻을 수 있는 index_to_char을 만듭니다."]},{"cell_type":"code","metadata":{"id":"h6MmdMVwH4CW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073181856,"user_tz":-540,"elapsed":16,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"eef8cd67-c37d-4646-d726-6c3d89bfe132"},"source":["index_to_char={}\n","for key, value in char_to_index.items():\n","    index_to_char[value] = key\n","print(index_to_char)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"]}]},{"cell_type":"markdown","metadata":{"id":"oh85JPUqH5Qi"},"source":["이제 입력 데이터와 레이블 데이터의 각 문자들을 정수로 맵핑합니다."]},{"cell_type":"code","metadata":{"id":"JPNKFnIfH62A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073181856,"user_tz":-540,"elapsed":14,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"15d24177-150c-4d8f-d977-285549831134"},"source":["x_data = [char_to_index[c] for c in input_str]\n","y_data = [char_to_index[c] for c in label_str]\n","print(x_data)\n","print(y_data)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 4, 4, 3, 2]\n","[4, 4, 3, 2, 0]\n"]}]},{"cell_type":"markdown","metadata":{"id":"oLqTb7dwH8Gq"},"source":["파이토치의 nn.RNN()은 기본적으로 3차원 텐서를 입력받습니다. 그렇기 때문에 배치 차원을 추가해줍니다."]},{"cell_type":"code","metadata":{"id":"JjrzV4IfH9sG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073181857,"user_tz":-540,"elapsed":14,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"cf16fa81-86ec-4d7c-b7af-186a042f7fec"},"source":["# 배치 차원 추가\n","# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n","x_data = [x_data]\n","y_data = [y_data]\n","print(x_data)\n","print(y_data)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 4, 4, 3, 2]]\n","[[4, 4, 3, 2, 0]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"Fwdjxc2fH-2f"},"source":["입력 시퀀스의 각 문자들을 원-핫 벡터로 바꿔줍니다."]},{"cell_type":"code","metadata":{"id":"46PuPmqlIABP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073181857,"user_tz":-540,"elapsed":13,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"91769c5b-1fdc-4263-dfe8-9777c6b43c43"},"source":["x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n","print(x_one_hot)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[array([[0., 1., 0., 0., 0.],\n","       [0., 0., 0., 0., 1.],\n","       [0., 0., 0., 0., 1.],\n","       [0., 0., 0., 1., 0.],\n","       [0., 0., 1., 0., 0.]])]\n"]}]},{"cell_type":"markdown","metadata":{"id":"6DCPxIkZIBLd"},"source":["입력 데이터와 레이블 데이터를 텐서로 바꿔줍니다."]},{"cell_type":"code","metadata":{"id":"BRVV3QuDICx-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073181857,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"967bfacc-dc8d-484f-e6ba-60f77c50b9ae"},"source":["X = torch.FloatTensor(x_one_hot)\n","Y = torch.LongTensor(y_data)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-c1bfbd518a63>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n","  X = torch.FloatTensor(x_one_hot)\n"]}]},{"cell_type":"markdown","metadata":{"id":"BvEnRtj_ID42"},"source":["이제 각 텐서의 크기를 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"RHRgFWNjIFel","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073181858,"user_tz":-540,"elapsed":10,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"4f746dc0-9fbf-4222-a34a-de828683b490"},"source":["print('훈련 데이터의 크기 : {}'.format(X.shape))\n","print('레이블의 크기 : {}'.format(Y.shape))"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 데이터의 크기 : torch.Size([1, 5, 5])\n","레이블의 크기 : torch.Size([1, 5])\n"]}]},{"cell_type":"markdown","metadata":{"id":"b70wYTBlIGuy"},"source":["## 2. 모델 구현하기"]},{"cell_type":"markdown","metadata":{"id":"E6xePH3rIIRR"},"source":["이제 RNN 모델을 구현해봅시다. 아래에서 fc는 완전 연결층(fully-connected layer)을 의미하며 출력층으로 사용됩니다."]},{"cell_type":"code","metadata":{"id":"jOFIXW0UIJ6Z","executionInfo":{"status":"ok","timestamp":1677073181858,"user_tz":-540,"elapsed":9,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["class Net(torch.nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(Net, self).__init__()\n","        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n","        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n","\n","    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n","        x, _status = self.rnn(x)\n","        x = self.fc(x)\n","        return x"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qLo-TzihILVV"},"source":["클래스로 정의한 모델을 net에 저장합니다."]},{"cell_type":"code","metadata":{"id":"sOT56KeMINBN","executionInfo":{"status":"ok","timestamp":1677073181858,"user_tz":-540,"elapsed":9,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["net = Net(input_size, hidden_size, output_size)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jJDtEWT0IOIL"},"source":["이제 입력된 모델에 입력을 넣어서 출력의 크기를 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"vhMKlsMSIP6Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073182186,"user_tz":-540,"elapsed":336,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"0c2a39c2-4b2f-4a39-d927-1d9f3a30e42b"},"source":["outputs = net(X)\n","print(outputs.shape) # 3차원 텐서"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 5, 5])\n"]}]},{"cell_type":"markdown","metadata":{"id":"-sX4Cjl3IRFr"},"source":["(1, 5, 5)의 크기를 가지는데 각각 배치 차원, 시점(timesteps), 출력의 크기입니다. 나중에 정확도를 측정할 때는 이를 모두 펼쳐서 계산하게 되는데, 이때는 view를 사용하여 배치 차원과 시점 차원을 하나로 만듭니다."]},{"cell_type":"code","metadata":{"id":"4r8EiecfISsU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073182186,"user_tz":-540,"elapsed":7,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"9bb7c727-7515-4745-bf3e-a1d5fd37e0c9"},"source":["print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 5])\n"]}]},{"cell_type":"markdown","metadata":{"id":"W6EKqtWGIUC8"},"source":["차원이 (5, 5)가 된 것을 볼 수 있습니다. 이제 레이블 데이터의 크기를 다시 복습봅시다."]},{"cell_type":"code","metadata":{"id":"phT2KQLIIVRD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073182186,"user_tz":-540,"elapsed":5,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"36b74c4c-6a85-447d-8a61-33886d74e0b2"},"source":["print(Y.shape)\n","print(Y.view(-1).shape)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 5])\n","torch.Size([5])\n"]}]},{"cell_type":"markdown","metadata":{"id":"MCMNGs1SIWWE"},"source":["레이블 데이터는 (1, 5)의 크기를 가지는데, 마찬가지로 나중에 정확도를 측정할 때는 이걸 펼쳐서 계산할 예정입니다. 이 경우 (5)의 크기를 가지게 됩니다. 이제 옵티마이저와 손실 함수를 정의합니다."]},{"cell_type":"code","metadata":{"id":"8W7Vat63IXrO","executionInfo":{"status":"ok","timestamp":1677073182187,"user_tz":-540,"elapsed":4,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["criterion = torch.nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), learning_rate)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hTFzsul6IZbz"},"source":["총 100번의 에포크를 학습합니다."]},{"cell_type":"code","metadata":{"id":"LVn20KIpIb7X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073183883,"user_tz":-540,"elapsed":1700,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"2928c7f8-212c-4ca0-a093-de647ad0e9a2"},"source":["for i in range(100):\n","    optimizer.zero_grad()\n","    outputs = net(X)\n","    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n","    loss.backward() # 기울기 계산\n","    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n","\n","    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n","    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n","    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n","    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["0 loss:  1.7082269191741943 prediction:  [[2 2 2 2 2]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  eeeee\n","1 loss:  1.4087722301483154 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","2 loss:  1.2318717241287231 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n","3 loss:  1.0097439289093018 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n","4 loss:  0.7836124897003174 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n","5 loss:  0.6091720461845398 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","6 loss:  0.47105956077575684 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","7 loss:  0.35608813166618347 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","8 loss:  0.2632216811180115 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","9 loss:  0.19063027203083038 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","10 loss:  0.1380091905593872 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","11 loss:  0.10527370125055313 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","12 loss:  0.0812133401632309 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","13 loss:  0.06093030050396919 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","14 loss:  0.04766392707824707 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","15 loss:  0.03902217000722885 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","16 loss:  0.032409779727458954 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","17 loss:  0.026834523305296898 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","18 loss:  0.02219396084547043 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","19 loss:  0.018532415851950645 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","20 loss:  0.015751687809824944 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","21 loss:  0.013649141415953636 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","22 loss:  0.012016303837299347 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","23 loss:  0.010691870003938675 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","24 loss:  0.00957354623824358 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","25 loss:  0.008607129566371441 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","26 loss:  0.007767674513161182 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","27 loss:  0.007042186800390482 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","28 loss:  0.006420041434466839 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","29 loss:  0.005889386869966984 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","30 loss:  0.005437393672764301 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","31 loss:  0.0050516435876488686 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","32 loss:  0.004720683209598064 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","33 loss:  0.004434981383383274 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","34 loss:  0.004186366219073534 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","35 loss:  0.0039680590853095055 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","36 loss:  0.003774947253987193 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","37 loss:  0.0036026090383529663 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","38 loss:  0.003447779221460223 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","39 loss:  0.003307776525616646 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","40 loss:  0.0031804819591343403 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","41 loss:  0.0030643190257251263 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","42 loss:  0.0029578988905996084 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","43 loss:  0.002860208973288536 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","44 loss:  0.002770307008177042 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","45 loss:  0.0026875329203903675 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","46 loss:  0.002611179370433092 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","47 loss:  0.0025407508946955204 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","48 loss:  0.0024756568018347025 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","49 loss:  0.002415519440546632 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","50 loss:  0.002359770704060793 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","51 loss:  0.0023081270046532154 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","52 loss:  0.0022600428201258183 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","53 loss:  0.002215330023318529 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","54 loss:  0.0021735369227826595 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","55 loss:  0.0021344744600355625 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","56 loss:  0.002097786869853735 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","57 loss:  0.0020631891675293446 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","58 loss:  0.002030539559200406 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","59 loss:  0.001999623840674758 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","60 loss:  0.0019701567944139242 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","61 loss:  0.0019420922035351396 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","62 loss:  0.0019152624299749732 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","63 loss:  0.001889573410153389 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","64 loss:  0.0018649058183655143 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","65 loss:  0.001841164892539382 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","66 loss:  0.0018182315398007631 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","67 loss:  0.0017961773555725813 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","68 loss:  0.0017747879028320312 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","69 loss:  0.001754111610352993 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","70 loss:  0.0017341235652565956 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","71 loss:  0.0017147297039628029 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","72 loss:  0.001695857965387404 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","73 loss:  0.0016775799449533224 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","74 loss:  0.0016598242800682783 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","75 loss:  0.0016425198409706354 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","76 loss:  0.0016257136594504118 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","77 loss:  0.001609358936548233 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","78 loss:  0.0015934312250465155 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","79 loss:  0.0015778361121192575 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","80 loss:  0.0015626440290361643 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","81 loss:  0.0015477845445275307 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","82 loss:  0.0015332571929320693 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","83 loss:  0.0015190623234957457 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","84 loss:  0.0015051520895212889 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","85 loss:  0.001491502858698368 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","86 loss:  0.001478114747442305 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","87 loss:  0.001464939909055829 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","88 loss:  0.0014520500553771853 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","89 loss:  0.001439350307919085 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","90 loss:  0.001426911330781877 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","91 loss:  0.0014146387111395597 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","92 loss:  0.0014025794807821512 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","93 loss:  0.0013907342217862606 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","94 loss:  0.0013790309894829988 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","95 loss:  0.0013675419613718987 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","96 loss:  0.00135621870867908 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","97 loss:  0.0013450381811708212 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","98 loss:  0.0013340950245037675 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n","99 loss:  0.0013232470955699682 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"]}]},{"cell_type":"markdown","metadata":{"id":"bl-WCxLbIfzG"},"source":["## 02. 문자 단위 RNN(Char RNN) - 더 많은 데이터\n"]},{"cell_type":"markdown","metadata":{"id":"FBFOOiFhIhx8"},"source":["이번 챕터에서는 더 많은 데이터 문자 단위 RNN을 구현합니다."]},{"cell_type":"markdown","metadata":{"id":"PYsmXbHhIiKA"},"source":["### 2. 문자 단위 RNN(Char RNN)\n"]},{"cell_type":"markdown","metadata":{"id":"InUKbGfkIjt0"},"source":["우선 필요한 도구들을 임포트합니다."]},{"cell_type":"code","metadata":{"id":"pIWL6r95IlEJ","executionInfo":{"status":"ok","timestamp":1677073183883,"user_tz":-540,"elapsed":17,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aIgh3vzZIoqW"},"source":["다음과 같이 임의의 샘플을 만듭니다."]},{"cell_type":"markdown","metadata":{"id":"nlELwUaVIpam"},"source":["### 1. 훈련 데이터 전처리하기\n"]},{"cell_type":"code","metadata":{"id":"YJ0gQqYZInm7","executionInfo":{"status":"ok","timestamp":1677073183884,"user_tz":-540,"elapsed":17,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["sentence = (\"if you want to build a ship, don't drum up people together to \"\n","            \"collect wood and don't assign them tasks and work, but rather \"\n","            \"teach them to long for the endless immensity of the sea.\")"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cJjwh_HlIs8f"},"source":["문자 집합을 생성하고, 각 문자에 고유한 정수를 부여합니다."]},{"cell_type":"code","metadata":{"id":"bEUfSHRhIuM_","executionInfo":{"status":"ok","timestamp":1677073183886,"user_tz":-540,"elapsed":19,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n","char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"lH0kCSKeIvmX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073183886,"user_tz":-540,"elapsed":19,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"55d2c251-a4e6-4376-9c33-18876a0e317e"},"source":["print(char_dic) # 공백도 여기서는 하나의 원소"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["{'l': 0, 'i': 1, '.': 2, 'p': 3, ' ': 4, 'g': 5, 't': 6, 'd': 7, 'm': 8, 'h': 9, \"'\": 10, 'e': 11, 'o': 12, 'y': 13, 'u': 14, 'f': 15, 'n': 16, 'w': 17, 'k': 18, ',': 19, 'a': 20, 's': 21, 'b': 22, 'r': 23, 'c': 24}\n"]}]},{"cell_type":"markdown","metadata":{"id":"sSouE8AiIxYo"},"source":["각 문자에 정수가 부여되었으며, 총 25개의 문자가 존재합니다. 문자 집합의 크기를 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"xJwfTdW6Ix8a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073183886,"user_tz":-540,"elapsed":14,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"0867ac66-c4e0-43b8-d8cc-291bfbae9f44"},"source":["dic_size = len(char_dic)\n","print('문자 집합의 크기 : {}'.format(dic_size))"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["문자 집합의 크기 : 25\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZV97EfZaI0Iv"},"source":["문자 집합의 크기는 25이며, 입력을 원-핫 벡터로 사용할 것이므로 이는 매 시점마다 들어갈 입력의 크기이기도 합니다. 이제 하이퍼파라미터를 설정합니다. hidden_size(은닉 상태의 크기)를 입력의 크기와 동일하게 줬는데, 이는 사용자의 선택으로 다른 값을 줘도 무방합니다.\n","\n","그리고 sequence_length라는 변수를 선언했는데, 우리가 앞서 만든 샘플을 10개 단위로 끊어서 샘플을 만들 예정이기 때문입니다. 이는 뒤에서 더 자세히 보겠습니다."]},{"cell_type":"code","metadata":{"id":"ws-XwAiVI1mD","executionInfo":{"status":"ok","timestamp":1677073183887,"user_tz":-540,"elapsed":14,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["# 하이퍼파라미터 설정\n","hidden_size = dic_size\n","sequence_length = 10  # 임의 숫자 지정\n","learning_rate = 0.1"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZjuKNyKEI2qL"},"source":["다음은 임의로 지정한 sequence_length 값인 10의 단위로 샘플들을 잘라서 데이터를 만드는 모습을 보여줍니다."]},{"cell_type":"code","metadata":{"id":"oLXi1Mz_I3_l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073183887,"user_tz":-540,"elapsed":13,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"8b8081e4-2730-48a4-9abb-e316d0abca67"},"source":["# 데이터 구성\n","x_data = []\n","y_data = []\n","\n","for i in range(0, len(sentence) - sequence_length):\n","    x_str = sentence[i:i + sequence_length]\n","    y_str = sentence[i + 1: i + sequence_length + 1]\n","    print(i, x_str, '->', y_str)\n","\n","    x_data.append([char_dic[c] for c in x_str])  # x str to index\n","    y_data.append([char_dic[c] for c in y_str])  # y str to index"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["0 if you wan -> f you want\n","1 f you want ->  you want \n","2  you want  -> you want t\n","3 you want t -> ou want to\n","4 ou want to -> u want to \n","5 u want to  ->  want to b\n","6  want to b -> want to bu\n","7 want to bu -> ant to bui\n","8 ant to bui -> nt to buil\n","9 nt to buil -> t to build\n","10 t to build ->  to build \n","11  to build  -> to build a\n","12 to build a -> o build a \n","13 o build a  ->  build a s\n","14  build a s -> build a sh\n","15 build a sh -> uild a shi\n","16 uild a shi -> ild a ship\n","17 ild a ship -> ld a ship,\n","18 ld a ship, -> d a ship, \n","19 d a ship,  ->  a ship, d\n","20  a ship, d -> a ship, do\n","21 a ship, do ->  ship, don\n","22  ship, don -> ship, don'\n","23 ship, don' -> hip, don't\n","24 hip, don't -> ip, don't \n","25 ip, don't  -> p, don't d\n","26 p, don't d -> , don't dr\n","27 , don't dr ->  don't dru\n","28  don't dru -> don't drum\n","29 don't drum -> on't drum \n","30 on't drum  -> n't drum u\n","31 n't drum u -> 't drum up\n","32 't drum up -> t drum up \n","33 t drum up  ->  drum up p\n","34  drum up p -> drum up pe\n","35 drum up pe -> rum up peo\n","36 rum up peo -> um up peop\n","37 um up peop -> m up peopl\n","38 m up peopl ->  up people\n","39  up people -> up people \n","40 up people  -> p people t\n","41 p people t ->  people to\n","42  people to -> people tog\n","43 people tog -> eople toge\n","44 eople toge -> ople toget\n","45 ople toget -> ple togeth\n","46 ple togeth -> le togethe\n","47 le togethe -> e together\n","48 e together ->  together \n","49  together  -> together t\n","50 together t -> ogether to\n","51 ogether to -> gether to \n","52 gether to  -> ether to c\n","53 ether to c -> ther to co\n","54 ther to co -> her to col\n","55 her to col -> er to coll\n","56 er to coll -> r to colle\n","57 r to colle ->  to collec\n","58  to collec -> to collect\n","59 to collect -> o collect \n","60 o collect  ->  collect w\n","61  collect w -> collect wo\n","62 collect wo -> ollect woo\n","63 ollect woo -> llect wood\n","64 llect wood -> lect wood \n","65 lect wood  -> ect wood a\n","66 ect wood a -> ct wood an\n","67 ct wood an -> t wood and\n","68 t wood and ->  wood and \n","69  wood and  -> wood and d\n","70 wood and d -> ood and do\n","71 ood and do -> od and don\n","72 od and don -> d and don'\n","73 d and don' ->  and don't\n","74  and don't -> and don't \n","75 and don't  -> nd don't a\n","76 nd don't a -> d don't as\n","77 d don't as ->  don't ass\n","78  don't ass -> don't assi\n","79 don't assi -> on't assig\n","80 on't assig -> n't assign\n","81 n't assign -> 't assign \n","82 't assign  -> t assign t\n","83 t assign t ->  assign th\n","84  assign th -> assign the\n","85 assign the -> ssign them\n","86 ssign them -> sign them \n","87 sign them  -> ign them t\n","88 ign them t -> gn them ta\n","89 gn them ta -> n them tas\n","90 n them tas ->  them task\n","91  them task -> them tasks\n","92 them tasks -> hem tasks \n","93 hem tasks  -> em tasks a\n","94 em tasks a -> m tasks an\n","95 m tasks an ->  tasks and\n","96  tasks and -> tasks and \n","97 tasks and  -> asks and w\n","98 asks and w -> sks and wo\n","99 sks and wo -> ks and wor\n","100 ks and wor -> s and work\n","101 s and work ->  and work,\n","102  and work, -> and work, \n","103 and work,  -> nd work, b\n","104 nd work, b -> d work, bu\n","105 d work, bu ->  work, but\n","106  work, but -> work, but \n","107 work, but  -> ork, but r\n","108 ork, but r -> rk, but ra\n","109 rk, but ra -> k, but rat\n","110 k, but rat -> , but rath\n","111 , but rath ->  but rathe\n","112  but rathe -> but rather\n","113 but rather -> ut rather \n","114 ut rather  -> t rather t\n","115 t rather t ->  rather te\n","116  rather te -> rather tea\n","117 rather tea -> ather teac\n","118 ather teac -> ther teach\n","119 ther teach -> her teach \n","120 her teach  -> er teach t\n","121 er teach t -> r teach th\n","122 r teach th ->  teach the\n","123  teach the -> teach them\n","124 teach them -> each them \n","125 each them  -> ach them t\n","126 ach them t -> ch them to\n","127 ch them to -> h them to \n","128 h them to  ->  them to l\n","129  them to l -> them to lo\n","130 them to lo -> hem to lon\n","131 hem to lon -> em to long\n","132 em to long -> m to long \n","133 m to long  ->  to long f\n","134  to long f -> to long fo\n","135 to long fo -> o long for\n","136 o long for ->  long for \n","137  long for  -> long for t\n","138 long for t -> ong for th\n","139 ong for th -> ng for the\n","140 ng for the -> g for the \n","141 g for the  ->  for the e\n","142  for the e -> for the en\n","143 for the en -> or the end\n","144 or the end -> r the endl\n","145 r the endl ->  the endle\n","146  the endle -> the endles\n","147 the endles -> he endless\n","148 he endless -> e endless \n","149 e endless  ->  endless i\n","150  endless i -> endless im\n","151 endless im -> ndless imm\n","152 ndless imm -> dless imme\n","153 dless imme -> less immen\n","154 less immen -> ess immens\n","155 ess immens -> ss immensi\n","156 ss immensi -> s immensit\n","157 s immensit ->  immensity\n","158  immensity -> immensity \n","159 immensity  -> mmensity o\n","160 mmensity o -> mensity of\n","161 mensity of -> ensity of \n","162 ensity of  -> nsity of t\n","163 nsity of t -> sity of th\n","164 sity of th -> ity of the\n","165 ity of the -> ty of the \n","166 ty of the  -> y of the s\n","167 y of the s ->  of the se\n","168  of the se -> of the sea\n","169 of the sea -> f the sea.\n"]}]},{"cell_type":"markdown","metadata":{"id":"9t6cV1ipI5Lz"},"source":["총 170개의 샘플이 생성되었습니다. 그리고 각 샘플의 각 문자들은 고유한 정수로 인코딩이 된 상태입니다. 첫번째 샘플의 입력 데이터와 레이블 데이터를 출력해봅시다."]},{"cell_type":"code","metadata":{"id":"70rB95ypI6UX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073183887,"user_tz":-540,"elapsed":12,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"a5a3eea0-64c7-46f6-b35f-763b85998b58"},"source":["print(x_data[0])\n","print(y_data[0])"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 15, 4, 13, 12, 14, 4, 17, 20, 16]\n","[15, 4, 13, 12, 14, 4, 17, 20, 16, 6]\n"]}]},{"cell_type":"markdown","metadata":{"id":"xnOfi4JLI77I"},"source":["한 칸씩 쉬프트 된 시퀀스가 정상적으로 출력되는 것을 볼 수 있습니다. 이제 입력 시퀀스에 대해서 원-핫 인코딩을 수행하고, 입력 데이터와 레이블 데이터를 텐서로 변환합니다."]},{"cell_type":"code","metadata":{"id":"-H861YS8I6Ya","executionInfo":{"status":"ok","timestamp":1677073183888,"user_tz":-540,"elapsed":10,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n","X = torch.FloatTensor(x_one_hot)\n","Y = torch.LongTensor(y_data)"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZwcQlU6OI996"},"source":["이제 훈련 데이터와 레이블 데이터의 크기를 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"0Wv8IFWDI_Ro","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073184244,"user_tz":-540,"elapsed":365,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"6644afbc-5a7e-4d5c-9b8a-2461dbbe1ac1"},"source":["print('훈련 데이터의 크기 : {}'.format(X.shape))\n","print('레이블의 크기 : {}'.format(Y.shape))"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 데이터의 크기 : torch.Size([170, 10, 25])\n","레이블의 크기 : torch.Size([170, 10])\n"]}]},{"cell_type":"markdown","metadata":{"id":"RDng_R_vJAkt"},"source":["원-핫 인코딩 된 결과를 보기 위해서 첫번째 샘플만 출력해봅시다."]},{"cell_type":"code","metadata":{"id":"qShtnSQ8JBtN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073184245,"user_tz":-540,"elapsed":15,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"b3e50114-9289-4e14-9d29-2d463ad84006"},"source":["print(X[0])"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0.]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"mNxBH8NnJEbG"},"source":["레이블 데이터의 첫번째 샘플도 출력해봅시다."]},{"cell_type":"code","metadata":{"id":"TmAMME5lJGSJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073184245,"user_tz":-540,"elapsed":14,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"825e8e47-aef8-466b-91b4-776e00278ecc"},"source":["print(Y[0])"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([15,  4, 13, 12, 14,  4, 17, 20, 16,  6])\n"]}]},{"cell_type":"markdown","metadata":{"id":"31zs8JjSJHW-"},"source":["### 2. 모델 구현하기\n"]},{"cell_type":"markdown","metadata":{"id":"rKyrdSkrJJEF"},"source":["모델은 앞서 실습한 문자 단위 RNN 챕터와 거의 동일합니다. 다만 이번에는 은닉층을 두 개 쌓을 겁니다."]},{"cell_type":"code","metadata":{"id":"2BEdUCUcJKST","executionInfo":{"status":"ok","timestamp":1677073184245,"user_tz":-540,"elapsed":12,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["class Net(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n","        super(Net, self).__init__()\n","        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n","        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n","\n","    def forward(self, x):\n","        x, _status = self.rnn(x)\n","        x = self.fc(x)\n","        return x"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"gZo4CLyLJK3O","executionInfo":{"status":"ok","timestamp":1677073184246,"user_tz":-540,"elapsed":12,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다."],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V2ji_7wRJNnd"},"source":["nn.RNN() 안에 num_layers라는 인자를 사용합니다. 이는 은닉층을 몇 개 쌓을 것인지를 의미합니다. 모델 선언 시 layers라는 인자에 2를 전달하여 은닉층을 두 개 쌓습니다. 비용 함수와 옵티마이저를 선언합니다."]},{"cell_type":"code","metadata":{"id":"OyQHDmjTJPDy","executionInfo":{"status":"ok","timestamp":1677073184246,"user_tz":-540,"elapsed":12,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["criterion = torch.nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), learning_rate)"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AJlsSbmoJP9j"},"source":["이제 모델에 입력을 넣어서 출력의 크기를 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"eZYI999JJM3Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073184246,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"45dc522e-c22d-4c56-f50b-ebd3a2f25a06"},"source":["outputs = net(X)\n","print(outputs.shape) # 3차원 텐서"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([170, 10, 25])\n"]}]},{"cell_type":"markdown","metadata":{"id":"9Dk1_STwJTdp"},"source":["(170, 10, 25)의 크기를 가지는데 각각 배치 차원, 시점(timesteps), 출력의 크기입니다. 나중에 정확도를 측정할 때는 이를 모두 펼쳐서 계산하게 되는데, 이때는 view를 사용하여 배치 차원과 시점 차원을 하나로 만듭니다."]},{"cell_type":"code","metadata":{"id":"2koPYnfwJSVP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073184247,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"8faa1c37-9d47-4dce-c491-34325f73a83c"},"source":["print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환."],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1700, 25])\n"]}]},{"cell_type":"markdown","metadata":{"id":"bf5cZlh8JVhL"},"source":["차원이 (1700, 25)가 된 것을 볼 수 있습니다. 이제 레이블 데이터의 크기를 다시 복습봅시다."]},{"cell_type":"code","metadata":{"id":"0pJy9v1TJWrk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073184247,"user_tz":-540,"elapsed":9,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"3f6a406e-e8c5-4407-9f96-e96574baaf1d"},"source":["print(Y.shape)\n","print(Y.view(-1).shape)"],"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([170, 10])\n","torch.Size([1700])\n"]}]},{"cell_type":"markdown","metadata":{"id":"fwYedIupJYJb"},"source":["레이블 데이터는 (170, 10)의 크기를 가지는데, 마찬가지로 나중에 정확도를 측정할 때는 이걸 펼쳐서 계산할 예정입니다. 이 경우 (1700)의 크기를 가지게 됩니다. 이제 옵티마이저와 손실 함수를 정의합니다."]},{"cell_type":"code","metadata":{"id":"RYNrO5odJZfB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073186689,"user_tz":-540,"elapsed":2450,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"f6b294f3-b371-4bbd-f8e5-a6877f509048"},"source":["for i in range(100):\n","    optimizer.zero_grad()\n","    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n","    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n","    loss.backward()\n","    optimizer.step()\n","\n","    # results의 텐서 크기는 (170, 10)\n","    results = outputs.argmax(dim=2)\n","    predict_str = \"\"\n","    for j, result in enumerate(results):\n","        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n","            predict_str += ''.join([char_set[t] for t in result])\n","        else: # 그 다음에는 마지막 글자만 반복 추가\n","            predict_str += char_set[result[-1]]\n","\n","    print(predict_str)"],"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["iiiiiiiiiiiiiiiiiiiiiiiiiiigiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n","eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n"," tnt tttnt ttt  ttttttttt   tttt  tt   ttt ttt  ttt ttt ttttttt tttt tttt ttt tttt  ttt  tt  tttt ttt ttt tttt tttt ttt  ttt ttt ttttttt t tt tttt tttttttttt ttttttttt  tttttttttt\n","                                                                                                                                                                                   \n","e eeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeee eeeeeeeeeeeeeee e eeeeeeeeeee eeeeeeeeeeeee e eee e eeeeeeeeeeeeee eeeeeeee eeeeeee eeeeeee e eeeeeeeeeeeeeeeeeeeeeee\n","t e t t  t t t e tttt t t t t tttttte t t tt t t t t t t t t te t tt t t tttt e e t t e t t e t tt tt t t e t e e e t t t t t tt t t t t tt t t e t t t e e e t t tt t t t t t t t \n","t t t tt t ttt tttttt t t t t tt t tttt t tt t t t t t t t t  t t ttttt ttt t t t ttt t t t t t  t t  t t t t t t t t  t  t t tt ttt t t  t t t t ttt t t t t t t tt t t t t t t t \n","  o      o                                                    o o                           o           o         o         o               o             o o                      \n","  o         o  o o        o o       o o o  o o o o   o o o o  o o        oooo o o o o o o o o o  o o  o o o   o o o o  oo o o  o o o o o  o o o       o o o o o    t o   o o o o o \n","         t              t t         t      t t t t     t t    t t  t      t t t t t t t t t t       t t t t   t t t     t t o  t     t t  t t     t t t t t t      t     t t t t   \n","  t t    t   t e  t t t t t t t t t e  t   e t t t t t t e et e t  t  t   t t t t t t t t t et e et e e t t   e t t t t t e et e   t e et e t   t t t e t t t      t     e e t t   \n"," et ee ete eet et e e t e e e eet e e  eee e e e e e e e etot e et e eete e e e eet e e eee et etet e e e ee  e e e t t e etot e  et etet e e e e e e e e e e eee ee eet e e e e e \n"," et ee e e eht et e e d e e e eethe et eeete e e e e eee etht e ee e eete e e e eethe e e e et etht e e e eet e e ehe ehe etht e  et etht ete e e ehe e e e e e e ee eht e ehe e ee\n"," et e  e d eh  u  d e t e e e eeth  e  e  t  e d e   eh  eth  e d  d te e e d e eeth  e e d ht etht e e d ee  e e d    he eth  d th  eth  e d e   eh  e d e e e e ed eh  e eh  e e \n","  to   e d th  u  t t t t t g   t t    t  t  t t d t  h  eto  t t  d to t t t g   t t t t t o  ttot t g d d   t g t t th  eto  d to  tto  e d t t to  t t d t t    t t     eh  t  t\n","  to lto t to  u  t t t   t d u t tout to to t t dot uo  tto  o t  d to t w t d ult t     tto   to  o g t d   t g t t to   to tt to   to  o t t t to  t t d t t  t t to  u uo  t  t\n","p ao lto d th  u  d d d   t dotlt toul to to t t dot uoa uto  a to doto   aot d tlt toa t dtoa  tot a w d do  a d t t toa uto  d tha  to  a d dot toa tot d d t  u d to  u aoa to  \n","p aot thtd tht u  d d d   d d t t t u  to d  t d dht tha  th  t dt doth d t d d t t t d t dth   ththt d d do  t d d totha  tht d th   th  atd dot th  d d d d t    d dot t th  do t\n","p e n  ond ahe es d t ton e d n't a et ep ee n s eon nhe  th  e  e d thn  a d e e t and e d he  ththe end aon d a d aothen th  d the  th  end eon the e d d d tn  nd eoe n ahe eon \n","c a e thnd the es d t doe d doe't a et ae rene e doe ehe eth  en e d thns aod aoe't anh e dthe  ththe end aon e and aothen the dothe eth  end aon the e d end t  etd eo  e ahe eon \n","c a e tend th  es d dndoe d doe't t et a' rene s don eheneth  e de d tens and aoe't tnd e dthe  tht'e and aen e dnd tothen the d the  th  end doe the dnd dnd te end eo  e ahe de e\n","c aoe tend to les d ando  s doe't t es t' t' ele donlthe  th  e ee d ten' tnd aoe't tnd   dtoe  tht'e and aen e dnd tothe  the d thel th  end doe the end end r  end eo  e the eo e\n","c toe tond the et e ando  e doe't t et tp to ele doe the  th  e  e d to s tnd t e't tns   dthe  thtke and ton e dot tothe  th  d the  th  end aoe the tnd enho   otd eh  e the to  \n","c t e tond th lut d anto  s doept t us tp ts e e dog the  th  o  e s tonp tnd a ept tnd rndtoe  thtke bnd aon e but tothe  the t thel th  end aot the end end rn otd eh  e the es  \n","c toe aond th lus d anto  d donpt tous ap uend endoglthem th lo kend aonp and aonpt and gndtoem thtke bnd aon e but tothen the h them th lond aon thempnd end tn ond gh le the pon \n","c toumtond th lut d dnth  d don't t us ap rend e dog them th lo ke d tons and aonpt tnd  ndthe  thtke aud aon e but tothe  the h them th lond aon the dnd end t  und eh le the dot \n","c t umuond to lus d dnto  d donpt t um ap uotd emton them th lo  ems tons and aonpt ans rndthem thtke and aon e but tothem the s toem th lond aon themtnd end tn und gh    themtotm\n","cot umtond to lut d dnto  e don't a um tp uo g emt r them th lo  emt woos and aon't ans rndthem th ke and aon e but tothem themh toem th lend aon themsod und tm undieh    themsotm\n","cot umtond to lutid anto me don't a ut a' uood e torlthem th lo lemt aoos and aon't antitndthem totke tnd aon , but tothem themh toem th lend aon themsnd enditm endith  e themsotm\n","cotoumtond woolut d dnto no don't aoum tp pooelemtor them to lo  emt aons and aon't anhirn'them totke pnd aon , but wothem toemh toem to lenp fon toempnd unditm undito  e toemponn\n","c togmtand thnlutld anth re don't aonm as aetgld th lthem th lh lt t want and aon't antitrsthem thtkt and aen d but tethem themt them thtle d ton themtnd gntitm endith  n themtttr\n","p to ltond th let d dod ggh bon't aoe  a' de e emto  them th lh  emh wo l and don't anh  p'them th ke bnd do  , but tathem themh ahe  th le ' don themdod e d tm erdith  e themdh  \n","p toe wond wo let o anshgpe bonpt aoe  ap aeon entog them to  e  ens wons ane aon't anhitn'toem tosk, and aon , but wothem tosmh toem to  end aon toempnd ensotn endito  e toenthsn\n","p aoe tond to lutid ansogpe don't anei ts wennlensogptoem to lo pent wons and aon't ansign'toem tosk, wnd aon , but tethem tosms toem to lend ton toempnd ensitn pndito  n toens,tn\n","p p g pond to lutid ansogp, don't a ui ts pensl gsogpthen to lh gent wond and aon't dnsiln'the  tosks wnd aon , but tethem tosmh toem to lend pon toempnd entiipmpndith  p toemp,in\n","p toumto d to lut d andh p, don't d um t' geodlh toglthem th lh lent wood and aon't and gn'them tosks and aook, but tothem ths h toem th lend don thempnd end igmgndity  e themdh m\n","c toumto d to lut d ansh pe don't a um t' aeod emtog them th la  emt tood and aon't ant gn'them thske tnd aoo , but tothem tos h toem th lend don thempnd umt tmmendity  e thempht \n","c toumuond to lut d anshtpe don't aoum ts aeos entog them to la  ent aood and aon't ant gnmthem tosks and aon , but tothem tosmh toem to land aon thempnd ent tnmen'ity  e toempot \n","p toemwond to but d anshipe don't aoum us penslensog then to bo  ent wond and aon't ansignsthem to ks and aon , but tather toemh toer to band pon toemdnd ent tnmendity  n toempht \n","l toeiwand to but d dnshipe don't aoum ps per'lh sog ther to bollens wand dnd aon't dnsignsthem to ks and aonk, but rothem toech them to band pon thempnd entiimmendith  p toempht \n","l doe wand to lui d ansh p, don't d um ap peo'lersog them to lal ert wond dnd aon't ansignmthe  to ks wnd aonk, but rothem toemh them to lanp pon the pnd ent immendity  p toemgh  \n","l toe want to lut d dnsh p, don't d um up p,ople tog them to lal emt wond dnd don't dssignmthem tosks wnd donk, but wothem tosmh them to land aon themend ent tmmendity  n toeme,o \n","l toemwond to lut d dnship, don't d um up p,oplemtog them to lal emt wond dnd don't dssignmthem tosks and donk, but wothem toemh them to lond aon themend ent tmmendity oe toeme,on\n","l toemwond th cutld anshtp, don't d um up peoplertog them th col e t wond and aon't dssign'them tosk, wnd aonk, but rothem thech toem to cond uon themend ent tmmensity oe toemehop\n","l aoe wont to cutld anshtp, don't a um up peoplertog ther to cal ert wond and aon't assignmthem to k, wnd wonk, but rothen toach toem to cond aon themsnd ent tmmensity oe toemehos\n","l aoe wont to cutld anshtp, don't a um up eeopldrtog ther to cal ert wond and aon't assignmthem tosks and aonk, but rother toach them to cand aon themend ens tmmensity on toere,op\n","l aoe wont to cutld anship, don't a um up eeoplertog ther to cal ert wond and aon't assign'them tosks and aonk, but rother toach them to cond aon themend ens tmmensity oe toeme,os\n","l aou wont to cuild anship, don't a um up eeoplemtog ther to col emt tord and aon't assignsthem tosks and aonk, but rother toach them to cone uon themsnd ens immensity of themeeos\n","p aou wont to build anship, don't drum up eeoplemtog ther to bol emt wond and aon't dssignmthem toskt and aonk, but rothen tosch them to bone uon thempnd ens immensity of theme,op\n","p uou wont to build anship, don't drum up eeoplemtog ther to bol emt word and aon't dssign ihem tosks and work, but rothen toach them to bone fon themend ess immensity of themeeop\n","p uoe wont to build anship, don't drum up eeoplemtog ther th bol emt word and don't dssign them tosks and wonk, but rother thach them to bong fon themend ess immensity of theme,op\n","p uou wont to cutld anship, don't drum up eeoplertog ther to col emt word and won't dssign them tosks wnd work, but rather toach them to cong fon themgnd ess immensity of themgeop\n","f aou ront to cutld anship, don't drum up feoplemtog ther to col emt word and don't dssign them tosks and aonk, but rather toach them to cong fon themend ess immensity of theme,op\n","l aou tont to cutld anship, don't drum up feoplemtog ther to collemt wood and don't dssign them tosks and aork, but rather toach them to cong fon themend ess immensity of themeoop\n","l aou wont to cuild anship, don't drum up feoplemtoglther to collemt wood and aon't dssignmthem tosks and aork, but rother toach them to cong fon themend ess immensity of themeeop\n","l aou wont to cuild anship, don't drum up feoplemtoglther to collemt wood and aon't dssign the  tosks and work, but rother toach the  to cong fon the end ess immensity of themeeop\n","l aou wont to cuild anship, don't drum up feoplemtogether to collert wood and don't dssign the  tosks and work, but rather thach the  to cong fon the end ess immensity of themeeop\n","l aou wont to build anship, don't drum up feoplemtogether to bollemt wood and don't dssign the  tosks and work, but rather thach the  to bong fon the end ess immensity of the eeos\n","l aou wont to build anship, don't drum up feoplemtoglther to bollemt wood and don't dssign the  tosks and work, but rother toach the  to bong fon the end ess immensity of themeeop\n","l aou wont to luild anship, don't drum up feoplemtogether to lollemt wood and don't dssign the  tosks and dork, but rather toach the  to long fon the end ess immensity of the eeop\n","f aou wont to luild anship, don't drum up feoplemtogether to lollemt wood and don't dssign the  tosks and work, but rather toach the  to long fon the end ess immensity of the eeop\n","f aou wont to luild anship, don't drum up feople together to lolle t wood and don't dssign the  tosks and work, but rather thach the  to long fon the end ess immensity of the eeop\n","f aoumwont to luild anship, don't drum up feople together to lolle t wood and don't dssign the  tosks and work, but rather toach the  to long fon the end ess immensity of the eeop\n","l aoumwont to luild anship, don't drum up feople together to lolle t wood and don't dssign the  tosks and work, but rather thach the  to long fon the end ess immensity of the eeop\n","l aoumwont to luild anship, don't drum up feople together to lolle t wood and don't dssign the  tosks and work, but rather thach the  to long fon the end ess immensity of themseos\n","l aoumwont to luild anship, don't drum up feople together to lollect wood and don't dssign the  tosks and work, but rather thach the  to long fon the end ess immensity of themeeop\n","l aou wont to luild anship, don't drum up feople together to lollect wood and don't dssign the  tosks and work, but rather thach the  to long for the end ess immensity of therseop\n","l aou wont to luild anship, don't drum up people together to lollect wood and don't dssign the  tosks and work, but rather thach the  to long for the end ess immensity of the eeop\n","s aou wont to luild anship, don't drum up people together to lollect wood and don't dssign the  tosks and work, but rather thach the  to long for the endless immentity of therseop\n","s aou want to luild anship, don't drum up feople together to lollect wood and don't dssign the  tosks and work, but rather thach the  to long fon the end ess immensity of the eerp\n","m aoe want to luild anship, don't arum up feople aogether th lolle t wood and don't dssign the  tosks andlwork, but rather thach the  th long for the endless immensity of the seop\n","l aoolwant to build anship, don't doum ae peonle dogether to bolle totood and wondt dsaign'the  tosks and work, but rathe  toobo the  ao bong for the gnd ess gm er'ity of toe goop\n","m toemuons th cuild ast ip, don t rrnm up feople togethem to csllect word and drn't rssignsthem tosks ansluork, but rathem tosch them to coss unn themunslessiimmensit t n themt, p\n","f toe uant ro 'uild dnship, don't drum up feoplertogether to lolee t wond and frn't assign'the  tosks and fork, but rather toach ther to lor' por the snd ens immendite  f thersh e\n","loroe aanh to cuildoankhip, don't dram tp aeoplentog ther to colee s wood and don't dsdigndthe  toske anh dork, but dathe  toach the  to lo gopor the end ess im end ty oe the e, e\n","l toe wont to cuil, askoip, dor't drum up aeoplestogfshe  to chllect tood and won't dsdiln'the  tosks and wook, but rathe  toach the  to co e fon the end essiimmendity of the e, e\n","l toeiwoni to cuild anthip, don't drum up aerplestogpsher th collust wood an' won't dssilnmthec toscs and work, but dathe  tosch them to co g fon themund essiimmensity of toamusop\n","foyoemwant to butld anthtp, don't drum up perplentogfther th collust word and won t dssign the  toscs and work, but dather tosch the  to cong for the ens ess tmmunsity on the e,an\n","f yoemwant to butld anship, don't drum pp peoplectogether th bollect word and don't dnsign'ther tosks and work, but rather thsch ther to bong for therens e s immentity of therehan\n","p eoe wont to build anship, don't drum pp deodle together to bolle t wood and don't dssign'ther tosss and work, but rather toach ther to bong for therend e s immensity of therehan\n","p eoemwont to build asship, don't drum up peoplectogether to bolle t wood and don't dssign'the  tosks and wook, but rather toach the  to lond fon the end e s immensity of the eeas\n","f yoemwant to luild asship, don't drum up people tog ther to lolle t wood and don't dssitn them toaks and wook, but rathe  toach them to lond fon themend ess immensity of theme,as\n","l aoemwant th luild ansyip, don't drum up people tog ther to lallect wood and aon't dssitn them toaks and wook, but rather toach them to long for themtnd ess immensity of thertyap\n","l aoe want th guild anship, don't drum up peoplectog them to collect wood and don't dssign the  toaks and wook, but rathem toach the  to long for the pnd ess immensity of themp,ap\n","l aoe want to luild anship, don't drum up peoplectogethe  to collect wood and aon't dssign the  tosks and work, but rather toach the  to con' for the end ess immensity of the enop\n","l aoe want to luild anship, don't drum up peoplectogethe  to collect wood and aon't dssign the  tosks and work, but rather toach the  to cong for the end ess immensity of the eeop\n","m aou want to luild anship, don't drum up people together to lollect wood and don't dssign the  tosks and work, but rather toach the  to long for the end ess immensity of the eeac\n","m aou want to build anship, don't drum up people together to bollect wood and don't dssign the  tosks and work, but rather toach ther to bong for the snd ess immensity of therseap\n","m aou want to build anship, don't arum up people together to bollect wood and don't assign them tosks and dork, but rather toach them to bong fon themend ess immensity of themeeop\n","f aoumwant to build anship, don't arum up people together to bollect wood and don't assign them tosks and work, but rather toach them to bong fon themend ess immensity of themeeop\n","f aoumwant to build a ship, don't drum up people together to bollect wood and won't assign them tosks and work, but rather toach them to bong for themend ess immensity of thereeap\n","f aou want to build a ship, don't drum up people together to bollect wood and won't dssign them tosks and work, but rather toach them to bong for themend ess immensity of themeeap\n","g you want to luild a ship, don't arum up people together to collect wood and don't assign the  tosks and work, but rather toach ther to cong for the end ess immensity of the seap\n","g you want to luild a ship, don't drum up people together to lollect wood and don't assign the  tosks and work, but rather toach the  to long for the end ess immensity of the eeap\n","g you want to luild a ship, don't drum up people together to lollect wood and won't dssign the  tosks and work, but rather toach the  to long for the end ess immensity of the eeap\n","f you want to luild a ship, don't arum up people together to lollect wood and won't assign the  tosks and work, but rather toach the  to long for the end ess immensity of the eeap\n","p you want to luild a ship, don't arum up people together to lollect wood and don't assign them tosks and work, but rather toach them to long for themend ess immensity of therseas\n","p you want to luild a ship, don't arum up people together to lollect wood and don't assign them tosks and work, but rather toach them to long for themend ess immensity of themeeas\n","p you want to build a ship, don't drum up people together to bollect wood and don't assign them tosks and work, but rather toach them to bong for themend ess immensity of themseap\n","p you want to luild a ship, don't arum up people together to lollect wood and don't assign them tosks and work, but rather toach them to long for themend ess immensity of themseas\n"]}]},{"cell_type":"markdown","metadata":{"id":"-55Cmg1EJbDO"},"source":["처음에는 이상한 예측을 하지만 마지막 에포크에서는 꽤 정확한 문자을 생성하는 것을 볼 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"ZtDNN469Jx5g"},"source":["# 03. 단어 단위 RNN - 임베딩 사용"]},{"cell_type":"markdown","metadata":{"id":"bH1BLyF8J3Z6"},"source":["이번 챕터에서는 문자 단위가 아니라 RNN의 입력 단위를 단어 단위로 사용합니다. 그리고 단어 단위를 사용함에 따라서 Pytorch에서 제공하는 임베딩 층(embedding layer)를 사용하겠습니다."]},{"cell_type":"markdown","metadata":{"id":"SpD9nC8EJ5Ba"},"source":["### 1. 훈련 데이터 전처리하기\n"]},{"cell_type":"markdown","metadata":{"id":"IffdcQq9J6eb"},"source":["우선 실습을 위해 필요한 도구들을 임포트합니다."]},{"cell_type":"code","metadata":{"id":"5RD70bgKJ67V","executionInfo":{"status":"ok","timestamp":1677073186689,"user_tz":-540,"elapsed":12,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H1liOjglJ8Mx"},"source":["실습을 위해 임의의 문장을 만듭니다."]},{"cell_type":"code","metadata":{"id":"orNpsH_aJ-AS","executionInfo":{"status":"ok","timestamp":1677073186689,"user_tz":-540,"elapsed":12,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["sentence = \"Repeat is the best medicine for memory\".split()"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ct7tyjPJ_I8"},"source":["우리가 만들 RNN은 'Repeat is the best medicine for'을 입력받으면 'is the best medicine for memory'를 출력하는 RNN입니다. 위의 임의의 문장으로부터 단어장(vocabulary)을 만듭니다."]},{"cell_type":"code","metadata":{"id":"1r9BIrxpKAV5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073186690,"user_tz":-540,"elapsed":13,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"b111e2e8-6226-425a-9d33-362292ad2948"},"source":["vocab = list(set(sentence))\n","print(vocab)"],"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["['memory', 'the', 'is', 'medicine', 'Repeat', 'best', 'for']\n"]}]},{"cell_type":"markdown","metadata":{"id":"QXCc6aQyKBpL"},"source":["이제 단어장의 단어에 고유한 정수 인덱스를 부여합니다. 그리고 그와 동시에 모르는 단어를 의미하는 UNK 토큰도 추가하겠습니다."]},{"cell_type":"code","metadata":{"id":"25cKmWS8KBhX","executionInfo":{"status":"ok","timestamp":1677073186690,"user_tz":-540,"elapsed":7,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["word2index = {tkn: i for i, tkn in enumerate(vocab, 1)}  # 단어에 고유한 정수 부여\n","word2index['<unk>']=0"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"waTq8t1IKGU3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073186690,"user_tz":-540,"elapsed":7,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"fa9f3093-3a01-4ad9-950d-f6f978b7f7c0"},"source":["print(word2index)"],"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["{'memory': 1, 'the': 2, 'is': 3, 'medicine': 4, 'Repeat': 5, 'best': 6, 'for': 7, '<unk>': 0}\n"]}]},{"cell_type":"markdown","metadata":{"id":"LV-enmMLKH6H"},"source":["이제 word2index가 우리가 사용할 최종 단어장인 셈입니다. word2index에 단어를 입력하면 맵핑되는 정수를 리턴합니다."]},{"cell_type":"code","metadata":{"id":"XuCOesMlKJAz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073186690,"user_tz":-540,"elapsed":5,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"b3f4d640-698a-4c5c-dbd8-c69517762929"},"source":["print(word2index['memory'])"],"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n"]}]},{"cell_type":"markdown","metadata":{"id":"Nlnfuc2iKKJJ"},"source":["단어 'memory'와 맵핑되는 정수는 2입니다. 예측 단계에서 예측한 문장을 확인하기 위해 idx2word도 만듭니다."]},{"cell_type":"code","metadata":{"id":"Mj5m7Sl3KMaI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073187178,"user_tz":-540,"elapsed":14,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"eb76a4e7-fdd9-4e11-e98c-d793287481c0"},"source":["# 수치화된 데이터를 단어로 바꾸기 위한 사전\n","index2word = {v: k for k, v in word2index.items()}\n","print(index2word)"],"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["{1: 'memory', 2: 'the', 3: 'is', 4: 'medicine', 5: 'Repeat', 6: 'best', 7: 'for', 0: '<unk>'}\n"]}]},{"cell_type":"markdown","metadata":{"id":"f_E8nmtGKKOV"},"source":["idx2word는 정수로부터 단어를 리턴하는 역할을 합니다. 정수 2를 넣어봅시다."]},{"cell_type":"code","metadata":{"id":"9DWTTlWXKOb7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073187178,"user_tz":-540,"elapsed":13,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"5675095e-8973-4d7d-954f-8540e1fc370e"},"source":["print(index2word[2])"],"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["the\n"]}]},{"cell_type":"markdown","metadata":{"id":"Y3Zsp0wFKPg4"},"source":["정수 2와 맵핑되는 단어는 memory인 것을 확인할 수 있습니다. 이제 데이터의 각 단어를 정수로 인코딩하는 동시에, 입력 데이터와 레이블 데이터를 만드는 build_data라는 함수를 만들어보겠습니다."]},{"cell_type":"code","metadata":{"id":"kDxlolmFKQBq","executionInfo":{"status":"ok","timestamp":1677073187179,"user_tz":-540,"elapsed":12,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["def build_data(sentence, word2index):\n","    encoded = [word2index[token] for token in sentence] # 각 문자를 정수로 변환. \n","    input_seq, label_seq = encoded[:-1], encoded[1:] # 입력 시퀀스와 레이블 시퀀스를 분리\n","    input_seq = torch.LongTensor(input_seq).unsqueeze(0) # 배치 차원 추가\n","    label_seq = torch.LongTensor(label_seq).unsqueeze(0) # 배치 차원 추가\n","    return input_seq, label_seq"],"execution_count":45,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TWxtOXhrKRe_"},"source":["만들어진 함수로부터 입력 데이터와 레이블 데이터를 얻습니다."]},{"cell_type":"code","metadata":{"id":"ulUZDdB_KStH","executionInfo":{"status":"ok","timestamp":1677073187179,"user_tz":-540,"elapsed":12,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["X, Y = build_data(sentence, word2index)"],"execution_count":46,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H4Okkud2KT9a"},"source":["입력 데이터와 레이블 데이터가 정상적으로 생성되었는지 출력해봅시다."]},{"cell_type":"code","metadata":{"id":"GhDA01imKXOL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073187179,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"7a52aee8-a613-4c6f-c5f6-ac06124a0811"},"source":["print(X)\n","print(Y)"],"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[5, 3, 2, 6, 4, 7]])\n","tensor([[3, 2, 6, 4, 7, 1]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"l9uSO3XjKYcG"},"source":["### 2. 모델 구현하기"]},{"cell_type":"markdown","metadata":{"id":"iZumEueXKbVg"},"source":["이제 모델을 설계합니다. 이전 모델들과 달라진 점은 임베딩 층을 추가했다는 점입니다. 파이토치에서는 nn.Embedding()을 사용해서 임베딩 층을 구현합니다. 임베딩층은 크게 두 가지 인자를 받는데 첫번째 인자는 단어장의 크기이며, 두번째 인자는 임베딩 벡터의 차원입니다."]},{"cell_type":"code","metadata":{"id":"gMeHajNGKc-8","executionInfo":{"status":"ok","timestamp":1677073187180,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["class Net(nn.Module):\n","    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n","        super(Net, self).__init__()\n","        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, # 워드 임베딩\n","                                            embedding_dim=input_size)\n","        self.rnn_layer = nn.RNN(input_size, hidden_size, # 입력 차원, 은닉 상태의 크기 정의\n","                                batch_first=batch_first)\n","        self.linear = nn.Linear(hidden_size, vocab_size) # 출력은 원-핫 벡터의 크기를 가져야함. 또는 단어 집합의 크기만큼 가져야함.\n","\n","    def forward(self, x):\n","        # 1. 임베딩 층\n","        # 크기변화: (배치 크기, 시퀀스 길이) => (배치 크기, 시퀀스 길이, 임베딩 차원)\n","        output = self.embedding_layer(x)\n","        # 2. RNN 층\n","        # 크기변화: (배치 크기, 시퀀스 길이, 임베딩 차원)\n","        # => output (배치 크기, 시퀀스 길이, 은닉층 크기), hidden (1, 배치 크기, 은닉층 크기)\n","        output, hidden = self.rnn_layer(output)\n","        # 3. 최종 출력층\n","        # 크기변화: (배치 크기, 시퀀스 길이, 은닉층 크기) => (배치 크기, 시퀀스 길이, 단어장 크기)\n","        output = self.linear(output)\n","        # 4. view를 통해서 배치 차원 제거\n","        # 크기변화: (배치 크기, 시퀀스 길이, 단어장 크기) => (배치 크기*시퀀스 길이, 단어장 크기)\n","        return output.view(-1, output.size(2))"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dEQK5Gl5KeUl"},"source":["이제 모델을 위해 하이퍼파라미터를 설정합니다."]},{"cell_type":"code","metadata":{"id":"DH-scZD4Kf9w","executionInfo":{"status":"ok","timestamp":1677073187180,"user_tz":-540,"elapsed":10,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["# 하이퍼 파라미터\n","vocab_size = len(word2index)  # 단어장의 크기는 임베딩 층, 최종 출력층에 사용된다. <unk> 토큰을 크기에 포함한다.\n","input_size = 5  # 임베딩 된 차원의 크기 및 RNN 층 입력 차원의 크기\n","hidden_size = 20  # RNN의 은닉층 크기"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Ib-vc4eKhAc"},"source":["모델을 생성합니다.\n"]},{"cell_type":"code","metadata":{"id":"YZD1tcalKiVd","executionInfo":{"status":"ok","timestamp":1677073187180,"user_tz":-540,"elapsed":10,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["# 모델 생성\n","model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n","# 손실함수 정의\n","loss_function = nn.CrossEntropyLoss() # 소프트맥스 함수 포함이며 실제값은 원-핫 인코딩 안 해도 됨.\n","# 옵티마이저 정의\n","optimizer = optim.Adam(params=model.parameters())"],"execution_count":50,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otvPGNKoKjgP"},"source":["모델에 입력을 넣어서 출력을 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"x6oHlD1CKkFh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073187180,"user_tz":-540,"elapsed":10,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"91d8b0f4-1147-4d7b-f900-5d5864d050cb"},"source":["# 임의로 예측해보기. 가중치는 전부 랜덤 초기화 된 상태이다.\n","output = model(X)\n","print(output)"],"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-1.2554e-01,  4.9404e-01, -4.5881e-01,  2.1254e-01, -1.5238e-02,\n","         -1.4731e-01, -1.4028e-01, -8.1226e-02],\n","        [-1.5630e-01,  3.4497e-01, -1.9386e-01,  1.9790e-01,  2.7031e-02,\n","         -1.1456e-02, -3.7134e-01, -7.8494e-02],\n","        [-3.0835e-01,  5.5323e-01, -1.0938e-01,  2.1897e-01,  4.3004e-01,\n","         -1.5360e-04, -1.3748e-01,  2.3569e-02],\n","        [-3.5201e-01,  5.6893e-01, -1.9633e-01,  2.3367e-01,  1.1566e-01,\n","         -1.3073e-01, -1.7868e-01, -1.5131e-01],\n","        [-1.0097e-01,  4.6204e-01, -2.9335e-01,  1.7501e-01,  1.7095e-01,\n","         -2.6435e-01, -3.6001e-02, -1.3005e-01],\n","        [-1.6307e-01,  2.1614e-01,  9.3140e-02, -2.0510e-03, -6.7697e-02,\n","          9.6937e-02, -4.9121e-01,  9.9841e-02]], grad_fn=<ViewBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"bcLqWmkLKmWH"},"source":["모델이 어떤 예측값을 내놓기는 하지만 현재 가중치는 랜덤 초기화되어 있어 의미있는 예측값은 아닙니다. 예측값의 크기를 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"KQg8PyMcKnEr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073187181,"user_tz":-540,"elapsed":9,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"73852cfa-9a0c-4da0-fba2-3bb3149a39df"},"source":["print(output.shape)"],"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([6, 8])\n"]}]},{"cell_type":"markdown","metadata":{"id":"ylpPMAMvKoTD"},"source":["예측값의 크기는 (6, 8)입니다. 이는 각각 (시퀀스의 길이, 은닉층의 크기)에 해당됩니다. 모델은 훈련시키기 전에 예측을 제대로 하고 있는지 예측된 정수 시퀀스를 다시 단어 시퀀스로 바꾸는 decode 함수를 만듭니다."]},{"cell_type":"code","metadata":{"id":"euAl0rfJKpxD","executionInfo":{"status":"ok","timestamp":1677073187181,"user_tz":-540,"elapsed":8,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}}},"source":["# 수치화된 데이터를 단어로 전환하는 함수\n","decode = lambda y: [index2word.get(x) for x in y]"],"execution_count":53,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mVFDDrv6KryV"},"source":["약 200 에포크 학습합니다."]},{"cell_type":"code","metadata":{"id":"lDu4YOx2Kssp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677073187987,"user_tz":-540,"elapsed":814,"user":{"displayName":"Hyun-Chul Kim","userId":"14185916921863952694"}},"outputId":"b46676ba-ec85-48fa-e36e-04bc8870042e"},"source":["# 훈련 시작\n","for step in range(201):\n","    # 경사 초기화\n","    optimizer.zero_grad()\n","    # 순방향 전파\n","    output = model(X)\n","    # 손실값 계산\n","    loss = loss_function(output, Y.view(-1))\n","    # 역방향 전파\n","    loss.backward()\n","    # 매개변수 업데이트\n","    optimizer.step()\n","    # 기록\n","    if step % 40 == 0:\n","        print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\n","        pred = output.softmax(-1).argmax(-1).tolist()\n","        print(\" \".join([\"Repeat\"] + decode(pred)))\n","        print()"],"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["[01/201] 2.0940 \n","Repeat memory memory memory memory memory memory\n","\n","[41/201] 1.5252 \n","Repeat is memory medicine medicine for memory\n","\n","[81/201] 0.8702 \n","Repeat is the best medicine for memory\n","\n","[121/201] 0.3941 \n","Repeat is the best medicine for memory\n","\n","[161/201] 0.1918 \n","Repeat is the best medicine for memory\n","\n","[201/201] 0.1113 \n","Repeat is the best medicine for memory\n","\n"]}]}]}